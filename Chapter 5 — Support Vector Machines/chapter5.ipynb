{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Support Vector Machines (SVM)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 1) What an SVM Is Trying to Do (Linear Case)\n",
        "\n",
        "We have a binary classification problem. A linear classifier draws a hyperplane:\n",
        "\n",
        "$$\\mathbf{w}^\\top \\mathbf{x} + b = 0$$\n",
        "\n",
        "The **sign** of $\\mathbf{w}^\\top \\mathbf{x} + b$ decides the class.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) The Key SVM Idea: Maximize the Margin\n",
        "\n",
        "* The **margin** is the \"street width\" between the two classes\n",
        "* SVM doesn't just find *any* separating hyperplane ‚Äî it finds the one with the **largest margin**, because:\n",
        "  * it usually generalizes better\n",
        "  * it is less sensitive to small shifts / noise\n",
        "\n",
        "**Support vectors:**\n",
        "* Only a few training points end up defining the boundary\n",
        "* These critical points are called **support vectors**\n",
        "* Move a non-support-vector ‚Üí boundary often unchanged\n",
        "* Move a support vector ‚Üí boundary can change\n",
        "\n",
        "> üìù **Key idea:** The entire model is determined by a small subset of training points ‚Äî the support vectors. The rest of the data is irrelevant once training is done.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Hard Margin vs Soft Margin\n",
        "\n",
        "**Hard-margin SVM:**\n",
        "* Assumes perfect separability (no overlaps)\n",
        "* Every point must be on the correct side with a minimum margin\n",
        "* **Problem:** real data is noisy ‚Üí hard-margin becomes brittle or impossible\n",
        "\n",
        "**Soft-margin SVM:**\n",
        "* Allows some points to be inside the margin, or even misclassified\n",
        "* Trades off: **wide margin** vs **few violations**\n",
        "* This tradeoff is controlled by $C$\n",
        "\n",
        "---\n",
        "\n",
        "### 4) The Hyperparameter $C$ (Most Important Practical Knob)\n",
        "\n",
        "> Think of $C$ as **\"how much we hate margin violations.\"**\n",
        "\n",
        "| $C$ value | Effect | Risk |\n",
        "|---|---|---|\n",
        "| **Large $C$** | Strongly penalizes violations ‚Üí narrow margin, fits training data closely | Overfit (sensitive to noise/outliers) |\n",
        "| **Small $C$** | Tolerates violations ‚Üí wide margin, more robust | Underfit (if too small) |\n",
        "\n",
        "$C$ behaves like an **inverse regularization strength**:\n",
        "* Smaller $C$ ‚Üí more regularization ‚Üí simpler boundary\n",
        "* Larger $C$ ‚Üí less regularization ‚Üí more complex boundary\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Why Feature Scaling Matters a Lot for SVMs\n",
        "\n",
        "SVMs use **distances and dot-products** heavily.\n",
        "\n",
        "* If one feature is 1000√ó larger than another, it can dominate the geometry entirely\n",
        "* The optimal hyperplane becomes distorted by scale\n",
        "\n",
        "> ‚úÖ In practice: **always use `StandardScaler` before fitting an SVM.**\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\",    LinearSVC(C=1.0, max_iter=10000))\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Key Takeaways ‚Äî Linear SVM\n",
        "\n",
        "| Concept | Core idea |\n",
        "|---|---|\n",
        "| Margin | \"Street width\" between classes ‚Äî SVM maximizes it |\n",
        "| Support vectors | The few points that define the boundary |\n",
        "| Hard margin | Perfect separation required ‚Äî brittle on real data |\n",
        "| Soft margin | Allows violations ‚Äî controlled by $C$ |\n",
        "| $C$ large | Narrow margin, low bias, high variance |\n",
        "| $C$ small | Wide margin, high bias, low variance |\n",
        "| Scaling | Always scale features before SVM |"
      ],
      "metadata": {
        "id": "0lZodhiLxa-8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y_STe4LvTBa",
        "outputId": "bbfca154-b476-4af4-d426-03a93e829548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Linear SVM (NO scaling), C=1.0\n",
            "  train acc: 1.0\n",
            "  test  acc: 1.0\n",
            "\n",
            "Linear SVM (WITH scaling), C=1.0\n",
            "  train acc: 1.0\n",
            "  test  acc: 1.0\n",
            "  # support vectors per class: [1 1]\n",
            "  total support vectors: 2\n",
            "\n",
            "Linear SVM (WITH scaling), C=0.1 (more regularization)\n",
            "  train acc: 1.0\n",
            "  test  acc: 1.0\n",
            "  # support vectors per class: [6 6]\n",
            "  total support vectors: 12\n",
            "\n",
            "Linear SVM (WITH scaling), C=100 (less regularization)\n",
            "  train acc: 1.0\n",
            "  test  acc: 1.0\n",
            "  # support vectors per class: [1 1]\n",
            "  total support vectors: 2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1) Load Iris and make it a binary problem: setosa (0) vs versicolor (1)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, (2, 3)]          # petal length, petal width (nice for SVM demos)\n",
        "y = iris.target\n",
        "\n",
        "mask = (y != 2)                   # drop virginica => keep classes 0 and 1\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# 2) Artificially create a scaling issue (multiply one feature)\n",
        "X_badscale = X.copy()\n",
        "X_badscale[:, 0] *= 1000.0        # petal length becomes huge-scale vs width\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_badscale, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "def eval_model(model, name):\n",
        "    model.fit(X_train, y_train)\n",
        "    pred_train = model.predict(X_train)\n",
        "    pred_test = model.predict(X_test)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"  train acc:\", accuracy_score(y_train, pred_train))\n",
        "    print(\"  test  acc:\", accuracy_score(y_test, pred_test))\n",
        "    if hasattr(model, \"named_steps\") and hasattr(model.named_steps.get(\"svc\", None), \"n_support_\"):\n",
        "        print(\"  # support vectors per class:\", model.named_steps[\"svc\"].n_support_)\n",
        "        print(\"  total support vectors:\", int(model.named_steps[\"svc\"].n_support_.sum()))\n",
        "\n",
        "# A) Linear SVM WITHOUT scaling (should suffer because we intentionally broke scaling)\n",
        "svm_no_scaling = SVC(kernel=\"linear\", C=1.0)\n",
        "eval_model(svm_no_scaling, \"Linear SVM (NO scaling), C=1.0\")\n",
        "\n",
        "# B) Linear SVM WITH scaling\n",
        "svm_scaled_C1 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"linear\", C=1.0))\n",
        "])\n",
        "eval_model(svm_scaled_C1, \"Linear SVM (WITH scaling), C=1.0\")\n",
        "\n",
        "# C) Effect of C: smaller vs larger (both scaled)\n",
        "svm_scaled_smallC = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"linear\", C=0.1))\n",
        "])\n",
        "eval_model(svm_scaled_smallC, \"Linear SVM (WITH scaling), C=0.1 (more regularization)\")\n",
        "\n",
        "svm_scaled_largeC = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"linear\", C=100.0))\n",
        "])\n",
        "eval_model(svm_scaled_largeC, \"Linear SVM (WITH scaling), C=100 (less regularization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Nonlinear SVMs + The Kernel Trick\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ C Controls \"Strictness\" ‚Äî Quick Lock-In\n",
        "\n",
        "> Lower $C$ ‚áí violations penalized less ‚áí wider margin ‚áí more points end up on/inside the margin ‚áí **more support vectors**\n",
        "\n",
        "* `C = 0.1` ‚Üí 12 support vectors (tolerant, wide margin)\n",
        "* `C = 1.0` ‚Üí 2 support vectors (clean separation)\n",
        "* `C = 100` ‚Üí 2 support vectors (strict, but data already separable)\n",
        "\n",
        "---\n",
        "\n",
        "## B) Nonlinear SVMs + The Kernel Trick\n",
        "\n",
        "### 1) Why We Need Kernels\n",
        "\n",
        "A linear SVM draws a **straight boundary** in the original feature space. But many datasets are not linearly separable.\n",
        "\n",
        "**Idea:**\n",
        "* Transform features into a **higher-dimensional space** where the data becomes separable\n",
        "* Then run a linear SVM there\n",
        "\n",
        "$$\\mathbf{w}^\\top \\phi(\\mathbf{x}) + b = 0$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) The Kernel Trick (The Practical Magic)\n",
        "\n",
        "In training/prediction, SVM mostly needs **dot products**:\n",
        "\n",
        "$$\\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)$$\n",
        "\n",
        "A **kernel function** computes this dot product **without explicitly computing** $\\phi(\\mathbf{x})$:\n",
        "\n",
        "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^\\top \\phi(\\mathbf{x}_j)$$\n",
        "\n",
        "> üìù **Key idea:** Kernels let us get nonlinear decision boundaries efficiently ‚Äî we never actually transform the data into the high-dimensional space. We just compute as *if* we did.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Polynomial Kernel (Smooth-ish Curved Boundaries)\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{z}) = (\\gamma\\, \\mathbf{x}^\\top \\mathbf{z} + r)^d$$\n",
        "\n",
        "In scikit-learn (`kernel=\"poly\"`):\n",
        "\n",
        "| Parameter | Effect |\n",
        "|---|---|\n",
        "| `degree` ($d$) | Higher = more complex curves |\n",
        "| `coef0` ($r$) | Shifts influence of higher-order vs lower-order terms |\n",
        "| `C` | Softness / regularization (same as linear case) |\n",
        "\n",
        "> **Intuition:** polynomial kernel creates feature interactions like $x_1^2$, $x_1 x_2$, etc. ‚Äî without explicitly computing them.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Gaussian RBF Kernel (Most Popular Default)\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{z}) = \\exp\\big(-\\gamma \\|\\mathbf{x} - \\mathbf{z}\\|^2\\big)$$\n",
        "\n",
        "Two key knobs: **$C$** and **$\\gamma$ (gamma)**\n",
        "\n",
        "---\n",
        "\n",
        "### 5) What $\\gamma$ Controls (Super Important)\n",
        "\n",
        "| $\\gamma$ value | Influence region | Boundary | Risk |\n",
        "|---|---|---|---|\n",
        "| **Small $\\gamma$** | Wide ‚Äî each point influences many others | Smooth, simple | Underfit |\n",
        "| **Large $\\gamma$** | Narrow ‚Äî each point influences only nearby points | Wiggly, complex | Overfit |\n",
        "\n",
        "> üìù **Intuition:** $\\gamma$ is like the \"reach\" of each training point. Small $\\gamma$ = each point \"votes\" over a wide area. Large $\\gamma$ = each point only affects its immediate neighborhood.\n",
        "\n",
        "---\n",
        "\n",
        "### 6) How $C$ and $\\gamma$ Interact\n",
        "\n",
        "| Combo | Effect |\n",
        "|---|---|\n",
        "| Large $C$ + Large $\\gamma$ | Very complex boundary ‚Üí **overfit** |\n",
        "| Small $C$ + Small $\\gamma$ | Very smooth boundary ‚Üí **underfit** |\n",
        "| Large $C$ + Small $\\gamma$ | Strict but smooth |\n",
        "| Small $C$ + Large $\\gamma$ | Tolerant but locally complex |\n",
        "\n",
        "> ‚úÖ In practice: tune $C$ and $\\gamma$ together (e.g., with `GridSearchCV`). They interact strongly.\n",
        "\n",
        "---\n",
        "\n",
        "### 7) Full Kernel Comparison\n",
        "\n",
        "| Kernel | Best when | Key params |\n",
        "|---|---|---|\n",
        "| `linear` | Data is (nearly) linearly separable | `C` |\n",
        "| `poly` | Smooth nonlinear boundary, known degree | `C`, `degree`, `coef0` |\n",
        "| `rbf` | General nonlinear ‚Äî **default first choice** | `C`, `gamma` |\n",
        "\n",
        "> üìù **Rule of thumb:** Start with RBF kernel. If data is high-dimensional and sparse, try linear first."
      ],
      "metadata": {
        "id": "hnFkRRLoziRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_moons(n_samples=400, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "def run(C, gamma):\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svc\", SVC(kernel=\"rbf\", C=C, gamma=gamma))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc  = accuracy_score(y_test,  model.predict(X_test))\n",
        "    sv = model.named_steps[\"svc\"].n_support_.sum()\n",
        "    print(f\"C={C:<6} gamma={gamma:<6} | train={train_acc:.3f} test={test_acc:.3f} | total SV={sv}\")\n",
        "\n",
        "print(\"RBF SVM: effect of gamma (C fixed)\")\n",
        "for g in [0.01, 0.1, 1, 10]:\n",
        "    run(C=1.0, gamma=g)\n",
        "\n",
        "print(\"\\nRBF SVM: interaction of C and gamma\")\n",
        "for C in [0.1, 1, 100]:\n",
        "    for g in [0.1, 10]:\n",
        "        run(C=C, gamma=g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMEZAZAdzjEz",
        "outputId": "d631a1b3-5d36-4426-c043-cd18c3403ae9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RBF SVM: effect of gamma (C fixed)\n",
            "C=1.0    gamma=0.01   | train=0.810 test=0.930 | total SV=173\n",
            "C=1.0    gamma=0.1    | train=0.820 test=0.950 | total SV=134\n",
            "C=1.0    gamma=1      | train=0.937 test=0.930 | total SV=87\n",
            "C=1.0    gamma=10     | train=0.960 test=0.900 | total SV=162\n",
            "\n",
            "RBF SVM: interaction of C and gamma\n",
            "C=0.1    gamma=0.1    | train=0.813 test=0.940 | total SV=187\n",
            "C=0.1    gamma=10     | train=0.950 test=0.920 | total SV=293\n",
            "C=1      gamma=0.1    | train=0.820 test=0.950 | total SV=134\n",
            "C=1      gamma=10     | train=0.960 test=0.900 | total SV=162\n",
            "C=100    gamma=0.1    | train=0.927 test=0.940 | total SV=84\n",
            "C=100    gamma=10     | train=0.993 test=0.930 | total SV=106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Choosing a Kernel + Tuning (C, Œ≥, degree, coef0)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Œ≥ Check Answer Lock-In\n",
        "\n",
        "* Increasing $\\gamma$ (with $C$ fixed) ‚Üí boundary becomes **more local / more wiggly**\n",
        "* Train accuracy rises, test accuracy can fall ‚Üí **classic overfitting risk**\n",
        "* Best test score usually at a **medium $\\gamma$** (e.g., 0.1) ‚Äî smooth enough to generalize\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Which Kernel Should I Try First?\n",
        "\n",
        "**Default choice: RBF kernel**\n",
        "* Works well for many problems\n",
        "* Models complex boundaries without hand-designing features\n",
        "\n",
        "**Polynomial kernel**\n",
        "* Good when the boundary is well-modeled by low-degree feature interactions (quadratic/cubic)\n",
        "* More interpretable than RBF in some settings (`degree` = \"complexity level\")\n",
        "\n",
        "> üìù **Rule of thumb:** No reason to prefer otherwise ‚Üí **start with RBF**. If you believe \"interactions up to degree 2/3 are enough\" ‚Üí try poly.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) What Hyperparameters Control Complexity?\n",
        "\n",
        "**RBF SVM:**\n",
        "\n",
        "| Parameter | Small value | Large value |\n",
        "|---|---|---|\n",
        "| $\\gamma$ | Smooth, global influence ‚Üí underfit | Very local, wiggly ‚Üí **overfit** |\n",
        "| $C$ | Wide margin, more violations ‚Üí regularized | Strict fit to training data ‚Üí **overfit** |\n",
        "\n",
        "> Overfit combo: **large $C$ + large $\\gamma$**\n",
        "> Underfit combo: **small $C$ + small $\\gamma$**\n",
        "\n",
        "**Polynomial SVM:**\n",
        "\n",
        "| Parameter | Role |\n",
        "|---|---|\n",
        "| `degree` ($d$) | Main complexity knob (2, 3, 4, ‚Ä¶) |\n",
        "| `coef0` ($r$) | Shifts influence of high-order vs low-order terms |\n",
        "| `C` | Soft margin ‚Äî same role as always |\n",
        "| `gamma` | Exists, but `degree` + `coef0` are the main \"feel\" controls |\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Interpreting Support Vectors (Very Practical)\n",
        "\n",
        "* **More support vectors** ‚Üí harder boundary to define (more points are close to / inside the margin)\n",
        "* At extreme $\\gamma$ values (very small or very large), support vector counts often rise ‚Äî the model geometry becomes \"awkward\" in different ways:\n",
        "  * Very small $\\gamma$ ‚Üí too smooth, many points near the margin\n",
        "  * Very large $\\gamma$ ‚Üí too local, many points individually influential\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Practical Tuning Workflow\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# RBF grid\n",
        "rbf_params = {\n",
        "    \"svm__C\":     [0.1, 1, 10, 100],\n",
        "    \"svm__gamma\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Polynomial grid\n",
        "poly_params = {\n",
        "    \"svm__C\":      [0.1, 1, 10],\n",
        "    \"svm__degree\": [2, 3, 4],\n",
        "    \"svm__coef0\":  [0, 1, 5]\n",
        "}\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\",    SVC(kernel=\"rbf\"))   # swap kernel=\"poly\" for poly search\n",
        "])\n",
        "\n",
        "grid_search = GridSearchCV(pipe, rbf_params, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid_search.best_params_)\n",
        "print(\"Best CV score:\", grid_search.best_score_)\n",
        "print(\"Test score:\", grid_search.score(X_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Full Tuning Reference\n",
        "\n",
        "| Kernel | Tune first | Then tune |\n",
        "|---|---|---|\n",
        "| `rbf` | `C`, `gamma` | ‚Äî |\n",
        "| `poly` | `degree`, `C` | `coef0` |\n",
        "| `linear` | `C` | ‚Äî |\n",
        "\n",
        "> ‚úÖ Always scale features with `StandardScaler` before any SVM kernel ‚Äî distances and dot products are scale-sensitive."
      ],
      "metadata": {
        "id": "pqixQnqe1cxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Same dataset as before\n",
        "X, y = make_moons(n_samples=400, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC())\n",
        "])\n",
        "\n",
        "# 1) RBF grid\n",
        "rbf_grid = {\n",
        "    \"svc__kernel\": [\"rbf\"],\n",
        "    \"svc__C\": [0.1, 1, 10, 100],\n",
        "    \"svc__gamma\": [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "# 2) Poly grid (keep it modest so it runs fast)\n",
        "poly_grid = {\n",
        "    \"svc__kernel\": [\"poly\"],\n",
        "    \"svc__C\": [0.1, 1, 10],\n",
        "    \"svc__degree\": [2, 3, 4],\n",
        "    \"svc__coef0\": [0, 1],\n",
        "    \"svc__gamma\": [\"scale\"],  # good default; keeps search smaller\n",
        "}\n",
        "\n",
        "def run_grid(param_grid, name):\n",
        "    gs = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
        "    gs.fit(X_train, y_train)\n",
        "\n",
        "    best = gs.best_estimator_\n",
        "    test_acc = best.score(X_test, y_test)\n",
        "    sv_total = int(best.named_steps[\"svc\"].n_support_.sum())\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"  best CV score:\", gs.best_score_)\n",
        "    print(\"  best params  :\", gs.best_params_)\n",
        "    print(\"  test score   :\", test_acc)\n",
        "    print(\"  total SV     :\", sv_total)\n",
        "\n",
        "run_grid(rbf_grid, \"RBF SVM\")\n",
        "run_grid(poly_grid, \"Polynomial SVM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiGZaZZM1iFn",
        "outputId": "4d302008-6c19-410f-ceca-4f21d6c7d21e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RBF SVM\n",
            "  best CV score: 0.9266666666666667\n",
            "  best params  : {'svc__C': 100, 'svc__gamma': 1, 'svc__kernel': 'rbf'}\n",
            "  test score   : 0.89\n",
            "  total SV     : 45\n",
            "\n",
            "Polynomial SVM\n",
            "  best CV score: 0.9299999999999999\n",
            "  best params  : {'svc__C': 10, 'svc__coef0': 1, 'svc__degree': 4, 'svc__gamma': 'scale', 'svc__kernel': 'poly'}\n",
            "  test score   : 0.92\n",
            "  total SV     : 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî SVM Regression (SVR): The Œµ-Insensitive Tube\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Kernel Wiggliness Lock-In\n",
        "\n",
        "* **RBF:** $\\gamma$ mainly controls wiggliness (local vs global influence); $C$ controls strictness (soft margin)\n",
        "* **Polynomial:** `degree` is the main shape complexity knob; `coef0` shifts higher-order vs lower-order term balance; $C$ controls strictness; $\\gamma$ matters if not left at `scale`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What SVR Tries to Do\n",
        "\n",
        "SVR is the SVM idea applied to **regression**.\n",
        "\n",
        "Instead of separating classes, SVR fits a function $f(\\mathbf{x})$ such that most points lie **inside a tube** around the prediction:\n",
        "\n",
        "* **Tube half-width = $\\varepsilon$ (epsilon)**\n",
        "* Point **inside** the tube ‚Üí **no penalty**\n",
        "* Point **outside** the tube ‚Üí penalized (grows with distance beyond $\\varepsilon$)\n",
        "\n",
        "This is the **$\\varepsilon$-insensitive loss:**\n",
        "\n",
        "| Region | Loss |\n",
        "|---|---|\n",
        "| $\\|y - f(\\mathbf{x})\\| \\leq \\varepsilon$ | $0$ |\n",
        "| $\\|y - f(\\mathbf{x})\\| > \\varepsilon$ | $\\|y - f(\\mathbf{x})\\| - \\varepsilon$ |\n",
        "\n",
        "> üìù **Key idea:** SVR doesn't care about errors as long as they're small enough (within the tube). This makes it robust to small noise.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) SVR Support Vectors (Key Insight)\n",
        "\n",
        "In SVR, the points that matter are:\n",
        "* Points **on the tube boundary**\n",
        "* Points **outside the tube**\n",
        "\n",
        "These become the **support vectors**.\n",
        "\n",
        "| $\\varepsilon$ size | Points outside tube | Support vectors | Model complexity |\n",
        "|---|---|---|---|\n",
        "| **Large $\\varepsilon$** (wide tube) | Few | Few | Simpler ‚Üí more bias |\n",
        "| **Small $\\varepsilon$** (narrow tube) | Many | Many | More complex ‚Üí more variance |\n",
        "\n",
        "---\n",
        "\n",
        "### 3) What $C$ Does in SVR\n",
        "\n",
        "Same principle as classification ‚Äî $C$ controls how much we punish points outside the tube:\n",
        "\n",
        "| $C$ value | Effect | Risk |\n",
        "|---|---|---|\n",
        "| **Large $C$** | Strongly penalize violations ‚Üí fits training points closely | Overfit |\n",
        "| **Small $C$** | Tolerates violations ‚Üí smoother model | Underfit |\n",
        "\n",
        "---\n",
        "\n",
        "### 4) RBF SVR: $\\gamma$ Still Matters\n",
        "\n",
        "Same as classification:\n",
        "* Larger $\\gamma$ ‚Üí more local ‚Üí wigglier regression curve\n",
        "* Smaller $\\gamma$ ‚Üí more global ‚Üí smoother curve\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Scaling\n",
        "\n",
        "> ‚úÖ SVR (especially RBF) is very sensitive to feature scale ‚Üí **always use `StandardScaler`**.\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Full SVR Parameter Reference\n",
        "\n",
        "| Parameter | Controls | Large value | Small value |\n",
        "|---|---|---|---|\n",
        "| $\\varepsilon$ | Tube width | Fewer SVs, simpler, more bias | More SVs, complex, more variance |\n",
        "| $C$ | Penalty for violations | Strict fit ‚Üí overfit risk | Tolerant ‚Üí underfit risk |\n",
        "| $\\gamma$ (RBF) | Local vs global influence | Wiggly ‚Üí overfit | Smooth ‚Üí underfit |\n",
        "\n",
        "---\n",
        "\n",
        "### 7) SVR vs SVM Classification ‚Äî Side by Side\n",
        "\n",
        "| | SVM Classification | SVR |\n",
        "|---|---|---|\n",
        "| Goal | Maximize margin between classes | Fit function within $\\varepsilon$-tube |\n",
        "| Support vectors | Points on/inside the margin | Points on/outside the tube |\n",
        "| $C$ role | Penalize margin violations | Penalize tube violations |\n",
        "| $\\varepsilon$ role | ‚Äî | Controls tube width (tolerance) |\n",
        "| Kernels | Linear, Poly, RBF | Same options |"
      ],
      "metadata": {
        "id": "-tR_ocdg3RFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Prefer modern sklearn RMSE function if available; otherwise fall back to sqrt(MSE)\n",
        "try:\n",
        "    from sklearn.metrics import root_mean_squared_error\n",
        "    def rmse(y_true, y_pred):\n",
        "        return root_mean_squared_error(y_true, y_pred)\n",
        "except Exception:\n",
        "    def rmse(y_true, y_pred):\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# 1D nonlinear regression dataset\n",
        "rng = np.random.RandomState(42)\n",
        "X = np.linspace(0, 6, 300).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + 0.25 * rng.randn(len(X))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "def eval_svr(C, gamma, epsilon):\n",
        "    model = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svr\", SVR(kernel=\"rbf\", C=C, gamma=gamma, epsilon=epsilon))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    pred_train = model.predict(X_train)\n",
        "    pred_test  = model.predict(X_test)\n",
        "\n",
        "    rmse_train = rmse(y_train, pred_train)\n",
        "    rmse_test  = rmse(y_test, pred_test)\n",
        "\n",
        "    n_sv = int(model.named_steps[\"svr\"].support_.shape[0])\n",
        "    print(f\"C={C:<6} gamma={gamma:<5} eps={epsilon:<4} | RMSE train={rmse_train:.3f} test={rmse_test:.3f} | SV={n_sv}\")\n",
        "\n",
        "print(\"Effect of epsilon (C and gamma fixed)\")\n",
        "for eps in [0.01, 0.1, 0.3, 0.7]:\n",
        "    eval_svr(C=10, gamma=1, epsilon=eps)\n",
        "\n",
        "print(\"\\nInteraction: epsilon with different C\")\n",
        "for C in [0.1, 1, 100]:\n",
        "    eval_svr(C=C, gamma=1, epsilon=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-JDV6Hp3R1D",
        "outputId": "8057f5b0-94bb-4860-cb99-f36ba8b9a6dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effect of epsilon (C and gamma fixed)\n",
            "C=10     gamma=1     eps=0.01 | RMSE train=0.255 test=0.217 | SV=220\n",
            "C=10     gamma=1     eps=0.1  | RMSE train=0.254 test=0.211 | SV=153\n",
            "C=10     gamma=1     eps=0.3  | RMSE train=0.257 test=0.206 | SV=57\n",
            "C=10     gamma=1     eps=0.7  | RMSE train=0.321 test=0.282 | SV=6\n",
            "\n",
            "Interaction: epsilon with different C\n",
            "C=0.1    gamma=1     eps=0.1  | RMSE train=0.263 test=0.206 | SV=160\n",
            "C=1      gamma=1     eps=0.1  | RMSE train=0.255 test=0.206 | SV=153\n",
            "C=100    gamma=1     eps=0.1  | RMSE train=0.253 test=0.217 | SV=155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Under the Hood of SVMs\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Œµ Lock-In\n",
        "\n",
        "* Larger $\\varepsilon$ = wider tube ‚Üí **fewer support vectors**, more bias, more tolerance\n",
        "* Smaller $\\varepsilon$ = narrower tube ‚Üí **more support vectors**, tighter fit, more variance\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Soft-Margin Linear SVM = \"Large Margin + Penalty for Violations\"\n",
        "\n",
        "A soft-margin linear SVM chooses $\\mathbf{w}, b$ by balancing:\n",
        "* **Large margin** (keep $\\|\\mathbf{w}\\|$ small)\n",
        "* **Few margin violations** (penalize points inside the margin / misclassified)\n",
        "\n",
        "$$\\min_{\\mathbf{w}, b}\\;\\;\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{i=1}^{m}\\max(0,\\; 1 - y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b))$$\n",
        "\n",
        "Where:\n",
        "* $y_i \\in \\{-1, +1\\}$\n",
        "* $\\max(0, 1 - y_i f(\\mathbf{x}_i))$ = the **hinge loss**\n",
        "* $C$ controls the tradeoff (strictness)\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Hinge Loss (What It \"Charges\" You For)\n",
        "\n",
        "Define the margin value:\n",
        "\n",
        "$$t_i = y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)$$\n",
        "\n",
        "| $t_i$ value | Situation | Loss |\n",
        "|---|---|---|\n",
        "| $t_i \\geq 1$ | Correctly classified, outside margin | $0$ |\n",
        "| $0 < t_i < 1$ | Correct side but inside margin | $> 0$ |\n",
        "| $t_i \\leq 0$ | Misclassified | $\\geq 1$ |\n",
        "\n",
        "> üìù **Key idea:** Only points **on/inside the margin** affect the objective. Points far away contribute nothing.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Why Support Vectors Are Special\n",
        "\n",
        "In the final solution:\n",
        "* Points **far from the boundary** ‚Üí zero hinge loss ‚Üí don't push on the boundary\n",
        "* Points **on the margin**, **inside the margin**, or **misclassified** ‚Üí non-zero hinge loss ‚Üí **these become support vectors**\n",
        "\n",
        "> The boundary is entirely defined by support vectors. Remove a non-support-vector and nothing changes. Move a support vector and the boundary shifts.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Dual Form (Why Kernels Work)\n",
        "\n",
        "In the dual formulation, the decision function becomes:\n",
        "\n",
        "$$f(\\mathbf{x}) = \\sum_{i \\in SV} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$$\n",
        "\n",
        "**Key takeaways:**\n",
        "* It's a **sum over support vectors only** ‚Äî non-SVs have $\\alpha_i = 0$\n",
        "* Data enters only via **kernel values** $K(\\mathbf{x}_i, \\mathbf{x})$\n",
        "* That's why we get nonlinear boundaries **without explicitly mapping features**\n",
        "\n",
        "For a linear kernel $K(\\mathbf{x}_i, \\mathbf{x}) = \\mathbf{x}_i^\\top \\mathbf{x}$, we can recover:\n",
        "\n",
        "$$\\mathbf{w} = \\sum_{i \\in SV} \\alpha_i y_i \\mathbf{x}_i$$\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Primal vs Dual ‚Äî Side by Side\n",
        "\n",
        "| | Primal | Dual |\n",
        "|---|---|---|\n",
        "| Variables | $\\mathbf{w}, b$ | $\\alpha_i$ (one per training point) |\n",
        "| Decision function | $\\mathbf{w}^\\top \\mathbf{x} + b$ | $\\sum_{i \\in SV} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$ |\n",
        "| Non-SVs | Zero hinge loss | $\\alpha_i = 0$ |\n",
        "| Kernels | Hard to apply | **Natural fit** |\n",
        "| Efficient when | Many instances, few features | Few instances, many features |\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Full Under-the-Hood Summary\n",
        "\n",
        "$$\\underbrace{\\frac{1}{2}\\|\\mathbf{w}\\|^2}_{\\text{maximize margin}} + \\underbrace{C \\sum_i \\max(0, 1 - y_i f(\\mathbf{x}_i))}_{\\text{penalize violations}}$$\n",
        "\n",
        "* **Hinge loss** = the \"charge\" for being on the wrong side of the margin\n",
        "* **Support vectors** = the only points with non-zero $\\alpha_i$ ‚Üí they alone define the boundary\n",
        "* **Kernel trick** = replace $\\mathbf{x}_i^\\top \\mathbf{x}$ with $K(\\mathbf{x}_i, \\mathbf{x})$ in the dual ‚Üí nonlinear boundaries for free"
      ],
      "metadata": {
        "id": "zuX7Ygm05_Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y01 = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
        "\n",
        "# Convert labels {0,1} -> {-1,+1} for hinge-loss style math\n",
        "y = np.where(y01 == 1, 1, -1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"linear\", C=1.0))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "svc = model.named_steps[\"svc\"]\n",
        "\n",
        "# Support vectors and dual coefficients\n",
        "SV = svc.support_vectors_                 # shape (n_SV, n_features)\n",
        "alpha_y = svc.dual_coef_.ravel()          # shape (n_SV,), equals alpha_i * y_i\n",
        "b = float(svc.intercept_[0])\n",
        "\n",
        "# Reconstruct w from dual: w = sum_i (alpha_i * y_i * x_i)\n",
        "w_dual = alpha_y @ SV                     # shape (n_features,)\n",
        "\n",
        "print(\"n_support_ per class:\", svc.n_support_, \" total:\", int(np.sum(svc.n_support_)))\n",
        "print(\"w from dual:\", w_dual)\n",
        "print(\"b:\", b)\n",
        "\n",
        "# Pick one test point and compare three equivalent computations\n",
        "x = model.named_steps[\"scaler\"].transform(X_test[:1])[0]   # IMPORTANT: compare in scaled space\n",
        "\n",
        "# 1) sklearn decision function (uses internal representation)\n",
        "sk_dec = float(svc.decision_function([x])[0])\n",
        "\n",
        "# 2) primal form: w^T x + b\n",
        "primal = float(w_dual @ x + b)\n",
        "\n",
        "# 3) dual form: sum_i (alpha_i y_i <x_i, x>) + b\n",
        "dual = float(alpha_y @ (SV @ x) + b)\n",
        "\n",
        "print(\"\\nOne test point comparison\")\n",
        "print(\"  sklearn decision:\", sk_dec)\n",
        "print(\"  primal  w^T x + b:\", primal)\n",
        "print(\"  dual sum over SV :\", dual)\n",
        "print(\"  abs differences  :\", abs(sk_dec - primal), abs(sk_dec - dual))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7cimdSj6AJr",
        "outputId": "e4bad858-6024-473c-9acb-a56a5973ab86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_support_ per class: [36 37]  total: 73\n",
            "w from dual: [ 0.758042   -1.46055609]\n",
            "b: -0.011180260556931257\n",
            "\n",
            "One test point comparison\n",
            "  sklearn decision: 0.9580109549633407\n",
            "  primal  w^T x + b: 0.9580109549633428\n",
            "  dual sum over SV : 0.9580109549633424\n",
            "  abs differences  : 2.1094237467877974e-15 1.7763568394002505e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî LinearSVC vs SVC(kernel=\"linear\") vs SGDClassifier(hinge)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Hinge Loss Lock-In\n",
        "\n",
        "* If $t_i = y_i f(\\mathbf{x}_i) \\geq 1$ ‚Üí hinge loss = **0** ‚Üí point does **not** become a support vector\n",
        "* Only points on/inside the margin ($t_i < 1$) have non-zero $\\alpha_i$ ‚Üí only they define the boundary\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Three Ways to Train a Linear SVM-Like Classifier\n",
        "\n",
        "**A) `SVC(kernel=\"linear\")`**\n",
        "* Full SVM class with linear kernel ‚Äî uses classic kernel SVM machinery\n",
        "* ‚úÖ Gives `support_vectors_`, easy to switch to RBF/poly later\n",
        "* ‚ùå Slow / memory-heavy on large $m$ (kernel-SVM methods don't scale well)\n",
        "\n",
        "> **Use when:** small/medium datasets, or when you want support vectors / might switch to kernels\n",
        "\n",
        "---\n",
        "\n",
        "**B) `LinearSVC`**\n",
        "* Specialized for linear SVMs ‚Äî no kernel trick\n",
        "* Optimized for large datasets and high-dimensional sparse features (text, TF-IDF)\n",
        "* ‚úÖ Much faster than `SVC(kernel=\"linear\")` on large $m$\n",
        "* ‚ùå No kernel trick, typically no `support_vectors_`\n",
        "\n",
        "> **Use when:** linear boundary is enough + lots of samples and/or many features\n",
        "\n",
        "---\n",
        "\n",
        "**C) `SGDClassifier(loss=\"hinge\")`**\n",
        "* Trains a linear classifier with hinge loss via stochastic gradient descent\n",
        "* Not the exact same optimizer as classic SVM solvers, but targets a similar objective\n",
        "* ‚úÖ Works for very large-scale / streaming data, supports `partial_fit`\n",
        "* ‚ùå More sensitive to tuning (learning rate, epochs, regularization), results can be noisier\n",
        "\n",
        "> **Use when:** massive dataset / streaming / need online updates\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Practical Decision Rule\n",
        "\n",
        "| Situation | Best choice |\n",
        "|---|---|\n",
        "| Want nonlinear boundary | `SVC(kernel=\"rbf\")` or `SVC(kernel=\"poly\")` |\n",
        "| Linear + big/sparse dataset (e.g. text) | `LinearSVC` |\n",
        "| Linear + huge/streaming data | `SGDClassifier(loss=\"hinge\")` |\n",
        "| Want support vectors / smaller dataset | `SVC(kernel=\"linear\")` |\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Under the Hood ‚Äî Why They Differ\n",
        "\n",
        "| | `SVC(kernel=\"linear\")` | `LinearSVC` | `SGDClassifier(hinge)` |\n",
        "|---|---|---|---|\n",
        "| Solver | libsvm (kernel SVM) | liblinear | SGD |\n",
        "| Scales with $m$ | ‚ùå Poor | ‚úÖ Good | ‚úÖ Excellent |\n",
        "| Kernel trick | ‚úÖ Yes | ‚ùå No | ‚ùå No |\n",
        "| `support_vectors_` | ‚úÖ Yes | ‚ùå No | ‚ùå No |\n",
        "| Online learning | ‚ùå No | ‚ùå No | ‚úÖ `partial_fit` |\n",
        "| Tuning sensitivity | Low | Low | **Higher** |\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Regularization Note\n",
        "\n",
        "All three regularize ‚Äî but the parameter names differ:\n",
        "\n",
        "| Classifier | Regularization param | Direction |\n",
        "|---|---|---|\n",
        "| `SVC` | `C` | Larger $C$ = less regularization |\n",
        "| `LinearSVC` | `C` | Larger $C$ = less regularization |\n",
        "| `SGDClassifier` | `alpha` | Larger `alpha` = more regularization |\n",
        "\n",
        "> üìù `SGDClassifier` uses `alpha` (like Ridge/Lasso), not `C`. Conceptually $\\alpha \\approx \\frac{1}{C}$."
      ],
      "metadata": {
        "id": "_WISbS9G7k8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# A moderately sized dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=12000, n_features=40, n_informative=15, n_redundant=5,\n",
        "    class_sep=1.2, flip_y=0.03, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "def eval_model(name, model):\n",
        "    t0 = time.perf_counter()\n",
        "    model.fit(X_train, y_train)\n",
        "    fit_s = time.perf_counter() - t0\n",
        "\n",
        "    pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "\n",
        "    extra = \"\"\n",
        "    # If it's an SVC in a pipeline, we can access support vectors\n",
        "    if hasattr(model, \"named_steps\") and \"svc\" in model.named_steps:\n",
        "        svc = model.named_steps[\"svc\"]\n",
        "        if hasattr(svc, \"support_\"):\n",
        "            extra = f\" | #SV={len(svc.support_)}\"\n",
        "\n",
        "    print(f\"{name:28s} fit={fit_s:.3f}s | test acc={acc:.3f}{extra}\")\n",
        "\n",
        "# 1) SVC linear (kernel SVM machinery)\n",
        "svc_linear = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"linear\", C=1.0))\n",
        "])\n",
        "\n",
        "# 2) LinearSVC (large-scale linear SVM)\n",
        "linearsvc = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LinearSVC(C=1.0, max_iter=20000))\n",
        "])\n",
        "\n",
        "# 3) SGDClassifier hinge (linear SVM-like)\n",
        "# Rough mapping: alpha ‚âà 1 / (C * n_samples) (useful heuristic)\n",
        "C = 1.0\n",
        "alpha = 1.0 / (C * len(X_train))\n",
        "sgd_hinge = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", SGDClassifier(loss=\"hinge\", alpha=alpha, max_iter=2000, tol=1e-3, random_state=42))\n",
        "])\n",
        "\n",
        "eval_model(\"SVC(kernel='linear')\", svc_linear)\n",
        "eval_model(\"LinearSVC\", linearsvc)\n",
        "eval_model(\"SGDClassifier(loss='hinge')\", sgd_hinge)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM5--Ll77ocu",
        "outputId": "667c0528-ee0a-473e-99f4-8c3f344cd8e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC(kernel='linear')         fit=21.858s | test acc=0.843 | #SV=3596\n",
            "LinearSVC                    fit=0.104s | test acc=0.843\n",
            "SGDClassifier(loss='hinge')  fit=0.610s | test acc=0.818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Nonlinear SVM: Polynomial Features vs Kernels\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ LinearSVC Check Lock-In\n",
        "\n",
        "For 2M samples + 100K sparse TF-IDF features ‚Üí **`LinearSVC`**\n",
        "* Scales well with large $m$ (liblinear solver)\n",
        "* Built for sparse, high-dimensional data\n",
        "* `SVC(kernel=\"linear\")` would be prohibitively slow; `SGDClassifier` needs more tuning\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Two Ways to Get a Nonlinear Decision Boundary\n",
        "\n",
        "**Method 1: Explicit Feature Expansion (Feature Engineering)**\n",
        "\n",
        "Transform input features into a richer set, then fit a linear model there.\n",
        "\n",
        "Example ‚Äî polynomial features:\n",
        "* Original: $(x_1, x_2)$\n",
        "* Add: $x_1^2, x_2^2, x_1 x_2, \\dots$\n",
        "* A **linear SVM in the expanded space** ‚Üí **nonlinear boundary in the original space**\n",
        "\n",
        "| | Pros | Cons |\n",
        "|---|---|---|\n",
        "| Explicit expansion | Simple mental model | Feature explosion ‚Üí slow |\n",
        "| Kernel trick | Powerful, no explicit mapping | Kernel SVMs slow on huge $m$ |\n",
        "\n",
        "---\n",
        "\n",
        "**Method 2: Kernels (Kernel Trick)**\n",
        "\n",
        "Instead of explicitly computing $\\phi(\\mathbf{x})$, the SVM uses:\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$$\n",
        "\n",
        "Nonlinear boundaries ‚Äî without explicitly building all features.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Polynomial Features vs Polynomial Kernel (Important Comparison)\n",
        "\n",
        "| Approach | How it works | Notes |\n",
        "|---|---|---|\n",
        "| `PolynomialFeatures` + `LinearSVC` | Explicitly creates polynomial terms ‚Üí linear model on expanded features | Transparent, but feature explosion at high degree |\n",
        "| `SVC(kernel=\"poly\")` | Implicitly computes polynomial dot products via kernel | No explicit feature matrix ‚Üí more memory efficient |\n",
        "\n",
        "> üìù They often behave similarly but are **not always identical** ‚Äî different solvers, scaling conventions, and regularization details.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) When to Use Which\n",
        "\n",
        "| Situation | Best approach |\n",
        "|---|---|\n",
        "| Small dataset, low degree | `PolynomialFeatures` + `LinearSVC` (interpretable) |\n",
        "| Medium dataset, unknown degree | `SVC(kernel=\"poly\")` with grid search on `degree` |\n",
        "| General nonlinear, medium dataset | `SVC(kernel=\"rbf\")` ‚Äî default first choice |\n",
        "| Very large dataset, nonlinear | Consider neural networks or ensemble methods |\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Nonlinearity Strategy Ladder\n",
        "\n",
        "$$\\text{Linear SVM} \\xrightarrow{\\text{not enough}} \\text{Poly Features + LinearSVC} \\xrightarrow{\\text{feature explosion}} \\text{Kernel SVM (poly/RBF)}$$\n",
        "\n",
        "> üìù **Key idea:** The kernel trick lets you work in an implicitly high-dimensional space **without the memory/compute cost** of explicitly building that space. This is the core power of kernel methods."
      ],
      "metadata": {
        "id": "ILPPAq-Z9RZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_moons(n_samples=2000, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "def eval_model(name, model):\n",
        "    t0 = time.perf_counter()\n",
        "    model.fit(X_train, y_train)\n",
        "    fit_s = time.perf_counter() - t0\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"{name:35s} fit={fit_s:.3f}s | test acc={acc:.3f}\")\n",
        "\n",
        "# 1) Linear SVM on raw features\n",
        "linear_raw = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LinearSVC(C=1.0, max_iter=20000))\n",
        "])\n",
        "\n",
        "# 2) Explicit polynomial features + linear SVM\n",
        "poly_features = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=3, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LinearSVC(C=1.0, max_iter=20000))\n",
        "])\n",
        "\n",
        "# 3) Polynomial kernel SVM\n",
        "poly_kernel = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"poly\", degree=3, C=1.0, coef0=1))\n",
        "])\n",
        "\n",
        "eval_model(\"LinearSVC (raw features)\", linear_raw)\n",
        "eval_model(\"PolyFeatures(deg=3)+LinearSVC\", poly_features)\n",
        "eval_model(\"SVC(poly kernel, deg=3)\", poly_kernel)\n",
        "\n",
        "# Also show how many features the polynomial expansion creates\n",
        "Xt = PolynomialFeatures(degree=3, include_bias=False).fit_transform(X_train[:1])\n",
        "print(\"\\n# features after degree-3 polynomial expansion:\", Xt.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ErOZnWN9V7L",
        "outputId": "1c9911e7-4fd0-4298-cdaa-09b7fca684ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC (raw features)            fit=0.010s | test acc=0.884\n",
            "PolyFeatures(deg=3)+LinearSVC       fit=0.017s | test acc=0.928\n",
            "SVC(poly kernel, deg=3)             fit=0.066s | test acc=0.926\n",
            "\n",
            "# features after degree-3 polynomial expansion: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî RBF SVM Tuning Playbook (C, Œ≥)\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Always Scale First\n",
        "\n",
        "Use `StandardScaler()` before fitting any SVM.\n",
        "\n",
        "> Without scaling, $\\gamma$ behaves unpredictably ‚Äî distances get distorted by feature scale differences.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Mental Model: What Each Knob Does\n",
        "\n",
        "**$\\gamma$ (gamma) = wiggliness / locality**\n",
        "\n",
        "| $\\gamma$ value | Influence region | Boundary | Risk |\n",
        "|---|---|---|---|\n",
        "| Small | Wide ‚Äî global | Smooth | Underfit |\n",
        "| Large | Narrow ‚Äî local | Very wiggly | Overfit |\n",
        "\n",
        "**$C$ = strictness about violations**\n",
        "\n",
        "| $C$ value | Margin | Violations | Risk |\n",
        "|---|---|---|---|\n",
        "| Small | Wide | Many allowed | Underfit |\n",
        "| Large | Narrow | Few tolerated | Overfit |\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Practical Search Strategy\n",
        "\n",
        "Start with a **coarse grid on powers of 10**:\n",
        "\n",
        "$$C \\in \\{0.1,\\ 1,\\ 10,\\ 100\\}$$\n",
        "$$\\gamma \\in \\{0.01,\\ 0.1,\\ 1,\\ 10\\}$$\n",
        "\n",
        "Pick the best combination by **cross-validation**.\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm\", SVC(kernel=\"rbf\"))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"svm__C\":     [0.1, 1, 10, 100],\n",
        "    \"svm__gamma\": [0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid_search.best_params_)\n",
        "print(\"Test score:\", grid_search.score(X_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Diagnose Quickly: Train vs Test Pattern\n",
        "\n",
        "| Pattern | Diagnosis | Fix |\n",
        "|---|---|---|\n",
        "| Train low + Test low | **Underfitting** | ‚Üë $C$ and/or ‚Üë $\\gamma$ |\n",
        "| Train high + Test noticeably lower | **Overfitting** | ‚Üì $C$ and/or ‚Üì $\\gamma$ |\n",
        "| Train high + Test high | ‚úÖ Good fit | Done |\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Full Tuning Loop (Mental Model)\n",
        "```\n",
        "Scale features\n",
        "    ‚Üì\n",
        "Coarse grid search (powers of 10)\n",
        "    ‚Üì\n",
        "Diagnose: underfit or overfit?\n",
        "    ‚Üì\n",
        "Refine grid around best region\n",
        "    ‚Üì\n",
        "Evaluate once on test set\n",
        "```\n",
        "\n",
        "> üìù **Key idea:** $C$ and $\\gamma$ interact ‚Äî always tune them **together**, not independently. The best combo is rarely at an extreme of either."
      ],
      "metadata": {
        "id": "rmUJbXkV_NGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svc\", SVC(kernel=\"rbf\"))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"svc__C\": [0.1, 1, 10, 100],\n",
        "    \"svc__gamma\": [0.01, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "print(gs.best_params_, gs.best_score_)\n",
        "print(\"test:\", gs.best_estimator_.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm89GNIj_N2K",
        "outputId": "28712a8c-c488-45d5-e287-76c7a3a869bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'svc__C': 1, 'svc__gamma': 1} 0.9486666666666667\n",
            "test: 0.934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî Support Vector Machines: Full Recap\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint References)\n",
        "\n",
        "| Section | Keywords |\n",
        "|---|---|\n",
        "| Large-Margin Classification | `Large margin` |\n",
        "| Soft Margin Classification | `C` |\n",
        "| Nonlinear SVM Classification | `Nonlinear` |\n",
        "| Polynomial Features | `polynomial`, `features` |\n",
        "| Kernel Trick | `Kernel Trick` |\n",
        "| Polynomial Kernel | `degree`, `coef0` |\n",
        "| Gaussian RBF Kernel | `gamma`, `RBF` |\n",
        "| Support Vector Regression | `SVR`, `Œµ-insensitive` |\n",
        "| Under the Hood | `hinge loss`, `dual` |\n",
        "| Large-Scale Linear SVM | `LinearSVC`, `SGDClassifier` |\n",
        "\n",
        "---\n",
        "\n",
        "## B) Chapter 5 Recap\n",
        "\n",
        "### 1) Big Idea: Large-Margin Classification\n",
        "\n",
        "A linear SVM finds a separating hyperplane:\n",
        "\n",
        "$$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$$\n",
        "\n",
        "Instead of just separating the classes, SVM **maximizes the margin** (the \"street width\" between classes).\n",
        "\n",
        "> Larger margins usually generalize better ‚Äî less sensitive to small data noise.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Support Vectors\n",
        "\n",
        "Only some training points determine the boundary: the **support vectors**.\n",
        "\n",
        "* Points **far from the boundary** ‚Üí usually don't affect the solution\n",
        "* Points **on/inside the margin** (or misclassified) ‚Üí **critical ‚Üí support vectors**\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Soft Margin + The Role of $C$\n",
        "\n",
        "Real datasets aren't perfectly separable ‚Üí we allow **margin violations**.\n",
        "\n",
        "| $C$ value | Effect | Risk |\n",
        "|---|---|---|\n",
        "| **Large $C$** | Penalize violations strongly ‚Üí strict fit | Overfit |\n",
        "| **Small $C$** | Tolerate violations ‚Üí wider margin | Underfit |\n",
        "\n",
        "> Smaller $C$ often increases the number of support vectors ‚Äî more points fall inside the margin.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Feature Scaling Is (Almost) Mandatory\n",
        "\n",
        "SVMs depend heavily on **dot-products / distances**. If features have different scales, one feature can dominate.\n",
        "\n",
        "> ‚úÖ Standard practice: `StandardScaler` ‚Üí SVM (always inside a `Pipeline`)\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Nonlinear SVMs: Two Routes\n",
        "\n",
        "**Route A: Polynomial Features + Linear Model**\n",
        "* Explicitly create new features (e.g., $x_1^2$, $x_1 x_2$, ‚Ä¶), then fit a linear SVM\n",
        "* ‚úÖ Straightforward idea | ‚ùå Feature space can blow up\n",
        "\n",
        "**Route B: Kernels (Kernel Trick)**\n",
        "* Don't explicitly build $\\phi(\\mathbf{x})$ ‚Äî use a kernel to compute:\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^\\top \\phi(\\mathbf{z})$$\n",
        "\n",
        "* Result: nonlinear boundary *as if* you mapped into a higher-dimensional space\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Polynomial Kernel (Key Knobs)\n",
        "\n",
        "Produces smooth nonlinear boundaries based on polynomial interactions.\n",
        "\n",
        "| Parameter | Effect |\n",
        "|---|---|\n",
        "| `degree` | Higher = more complex curve |\n",
        "| `coef0` | Shifts influence of higher-order vs lower-order terms |\n",
        "| `C` | Soft margin strictness (same as always) |\n",
        "\n",
        "---\n",
        "\n",
        "### 7) RBF (Gaussian) Kernel ‚Äî Most Common Default\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{z}) = \\exp(-\\gamma \\|\\mathbf{x} - \\mathbf{z}\\|^2)$$\n",
        "\n",
        "| Parameter | Small value | Large value |\n",
        "|---|---|---|\n",
        "| $\\gamma$ | Wide influence ‚Üí smooth ‚Üí **underfit** | Local influence ‚Üí wiggly ‚Üí **overfit** |\n",
        "| $C$ | More regularization ‚Üí tolerant | Stricter fit ‚Üí **overfit** |\n",
        "\n",
        "**Common patterns:**\n",
        "* Overfit: **large $C$ + large $\\gamma$**\n",
        "* Underfit: **small $C$ + small $\\gamma$**\n",
        "\n",
        "---\n",
        "\n",
        "### 8) Simple Tuning Loop (Chapter-Level Playbook)\n",
        "\n",
        "1. **Scale features** (`StandardScaler`)\n",
        "2. **Coarse grid** (powers of 10):\n",
        "\n",
        "$$C \\in \\{0.1,\\ 1,\\ 10,\\ 100\\}, \\quad \\gamma \\in \\{0.01,\\ 0.1,\\ 1,\\ 10\\}$$\n",
        "\n",
        "3. **Diagnose:**\n",
        "\n",
        "| Pattern | Diagnosis | Fix |\n",
        "|---|---|---|\n",
        "| Train low + Test low | Underfit | ‚Üë $C$ and/or ‚Üë $\\gamma$ |\n",
        "| Train high + Test lower | Overfit | ‚Üì $C$ and/or ‚Üì $\\gamma$ |\n",
        "\n",
        "---\n",
        "\n",
        "### 9) SVR: The Œµ-Insensitive Tube\n",
        "\n",
        "Goal: fit a function where points **inside a tube of width $\\varepsilon$** incur **zero loss**.\n",
        "\n",
        "| Parameter | Large value | Small value |\n",
        "|---|---|---|\n",
        "| $\\varepsilon$ | Wider tube ‚Üí fewer SVs ‚Üí simpler | Tighter tube ‚Üí more SVs ‚Üí complex |\n",
        "| $C$ | Fits strictly | Smooth, tolerant |\n",
        "| $\\gamma$ (RBF) | Wiggly curve | Smooth curve |\n",
        "\n",
        "---\n",
        "\n",
        "### 10) Under the Hood: Hinge Loss + Dual View\n",
        "\n",
        "Soft-margin SVM uses **hinge loss** ‚Äî points correctly classified beyond the margin contribute **0 loss**.\n",
        "\n",
        "Decision function = **sum over support vectors only**:\n",
        "\n",
        "$$f(\\mathbf{x}) = \\sum_{i \\in SV} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$$\n",
        "\n",
        "> This is why kernels work: the model only needs kernel values $K(\\cdot, \\cdot)$, not explicit $\\phi(\\mathbf{x})$.\n",
        "\n",
        "---\n",
        "\n",
        "### 11) Linear SVM Implementations: When to Use Which\n",
        "\n",
        "| Classifier | Best when | Key advantage |\n",
        "|---|---|---|\n",
        "| `SVC(kernel=\"linear\")` | Small/medium datasets | Exposes support vectors explicitly |\n",
        "| `LinearSVC` | Large datasets, sparse features (TF-IDF) | Optimized for large $m$ |\n",
        "| `SGDClassifier(loss=\"hinge\")` | Very large / streaming / online learning | Supports `partial_fit` |"
      ],
      "metadata": {
        "id": "qenNCONk_rpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 5 ‚Äî SVM Exercises: Q&A\n",
        "\n",
        "---\n",
        "\n",
        "**Q1. What is the fundamental idea behind Support Vector Machines?**\n",
        "\n",
        "An SVM tries to find a decision boundary (hyperplane) that separates classes while **maximizing the margin** (the distance to the nearest training points). This \"largest-margin\" choice tends to generalize better. With a **soft margin**, it also allows some violations while balancing them against a wider margin using the hyperparameter $C$.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. What is a support vector?**\n",
        "\n",
        "A support vector is a training instance that lies **on the margin boundary or inside the margin** (including misclassified points), meaning it directly influences the position of the decision boundary. Points far from the boundary typically have **no effect** on the final model.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Why is it important to scale the inputs when using SVMs?**\n",
        "\n",
        "SVMs rely heavily on **dot products and distances** (especially with kernels like RBF). If features are on very different scales, large-scale features dominate the geometry ‚Äî distorting margins and kernel similarities. Scaling (e.g., standardization) makes each feature contribute more fairly and usually improves both performance and stability.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?**\n",
        "\n",
        "* **Confidence score:** Yes ‚Äî an SVM naturally outputs a **decision score** (signed distance to the decision boundary), which serves as a confidence-like ranking.\n",
        "* **Probability:** Not natively. True probabilities require extra **probability calibration** (e.g., Platt scaling or isotonic regression), which adds a fitting step and may trade some speed for probabilistic outputs.\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. Should you use the primal or the dual form to train a model on millions of instances and hundreds of features?**\n",
        "\n",
        "With **millions of instances** and only **hundreds of features**, prefer the **primal** (or primal-like) approach ‚Äî the dual typically scales poorly with the number of training instances. In practice this means using efficient **linear SVM solvers** (`LinearSVC`) or **SGD-based hinge-loss training** (`SGDClassifier`) rather than a full kernelized dual formulation.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. You trained an RBF SVM and it underfits. Should you increase or decrease Œ≥? What about C?**\n",
        "\n",
        "Underfitting = model too simple ‚Üí **increase both**:\n",
        "* **Increase $\\gamma$** ‚Üí more flexible/local boundary\n",
        "* **Increase $C$** ‚Üí reduce regularization, penalize violations more strongly\n",
        "\n",
        "‚ö†Ô∏è Adjust gradually ‚Äî too-large $\\gamma$ or $C$ can quickly cause overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. How should you set the QP parameters (H, f, A, b) to solve the soft-margin linear SVM using an off-the-shelf QP solver?**\n",
        "\n",
        "Let the variable vector be $\\boldsymbol{\\theta} = [\\mathbf{w};\\ b;\\ \\boldsymbol{\\xi}]$ where $\\mathbf{w} \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}$, and $\\boldsymbol{\\xi} \\in \\mathbb{R}^m$.\n",
        "\n",
        "| QP param | Setting |\n",
        "|---|---|\n",
        "| $\\mathbf{H}$ | Block matrix: identity on the $\\mathbf{w}$-block, zeros elsewhere ‚Üí quadratic term = $\\frac{1}{2}\\|\\mathbf{w}\\|^2$ |\n",
        "| $\\mathbf{f}$ | Zeros for $\\mathbf{w}$ and $b$; $C$ for each slack $\\xi_i$ ‚Üí linear term = $C\\sum \\xi_i$ |\n",
        "| $\\mathbf{A}, \\mathbf{b}$ | Encode constraints $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1 - \\xi_i$ and $\\xi_i \\geq 0$ into the solver's inequality form |\n",
        "\n",
        "> The exact sign/stacking depends on whether your solver expects $A\\boldsymbol{\\theta} \\leq \\mathbf{b}$ or $A\\boldsymbol{\\theta} \\geq \\mathbf{b}$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Q8. Train a LinearSVC, SVC, and SGDClassifier on the same linearly separable dataset. Can you get them to produce roughly the same model?**\n",
        "\n",
        "Yes ‚Äî use the same scaling, set all three to optimize a **linear decision boundary**:\n",
        "* `SVC(kernel=\"linear\")`\n",
        "* `LinearSVC` (as-is)\n",
        "* `SGDClassifier(loss=\"hinge\")`\n",
        "\n",
        "Tune their regularization so strengths are comparable (conceptually: $C$ in SVC/LinearSVC ‚Üî $\\frac{1}{\\alpha \\cdot n}$ in SGDClassifier). For SGD, ensure enough iterations and a stable learning-rate schedule.\n",
        "\n",
        "If done correctly, all three should yield **very similar decision boundaries and accuracy** ‚Äî with `LinearSVC` and `SGDClassifier` usually training much faster than `SVC` on large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "**Q9. Train an SVM classifier on the MNIST dataset. What accuracy can you reach?**\n",
        "\n",
        "* **Linear SVM (one-vs-rest):** typically reaches low-to-mid **90%** accuracy fairly quickly\n",
        "* **RBF-kernel SVM** with careful tuning of $C$ and $\\gamma$: can often reach **97‚Äì98%**, but training becomes significantly slower and memory-heavier\n",
        "\n",
        "> ‚úÖ Practical approach: use a coarse-to-fine grid search on a smaller validation set first, then confirm on the full test set. Scaling is essential ‚Äî always `StandardScaler` before fitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Q10. Train an SVM regressor on the California housing dataset.**\n",
        "\n",
        "Workflow:\n",
        "1. **Standardize features** (`StandardScaler`) ‚Äî SVR is very sensitive to scale\n",
        "2. **Baseline:** `LinearSVR` ‚Äî fast, establishes a floor\n",
        "3. **Nonlinear:** `SVR(kernel=\"rbf\")` ‚Äî tune $C$, $\\gamma$, and $\\varepsilon$ via cross-validation\n",
        "4. **Evaluate** with RMSE (or MAE)\n",
        "\n",
        "Expected outcome: RBF SVR captures nonlinearities better than the linear baseline when tuned well, though it is slower and more sensitive to hyperparameters and scaling.\n",
        "\n",
        "| Model | Speed | Nonlinearity | Sensitivity |\n",
        "|---|---|---|---|\n",
        "| `LinearSVR` | Fast | ‚ùå No | Low |\n",
        "| `SVR(kernel=\"rbf\")` | Slower | ‚úÖ Yes | High ‚Äî tune carefully |"
      ],
      "metadata": {
        "id": "U84Q-_JVATup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8) LinearSVC vs SVC(linear) vs SGDClassifier on a linearly separable dataset"
      ],
      "metadata": {
        "id": "wQWwSPxmCNu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1) Make a (nearly) linearly separable dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=6000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=0,\n",
        "    n_clusters_per_class=1,\n",
        "    class_sep=2.5,      # higher => more separable\n",
        "    flip_y=0.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Scale (important for SVMs)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "# 3) Train the three linear models\n",
        "C = 1000.0  # large C ~ close to hard-margin if separable\n",
        "\n",
        "svc = SVC(kernel=\"linear\", C=C)\n",
        "svc.fit(X_train_s, y_train)\n",
        "\n",
        "linearsvc = LinearSVC(C=C, max_iter=20000)\n",
        "linearsvc.fit(X_train_s, y_train)\n",
        "\n",
        "# Heuristic mapping: alpha ‚âà 1 / (C * n_samples)\n",
        "alpha = 1.0 / (C * len(X_train_s))\n",
        "sgd = SGDClassifier(\n",
        "    loss=\"hinge\",\n",
        "    alpha=alpha,\n",
        "    max_iter=5000,\n",
        "    tol=1e-4,\n",
        "    random_state=42\n",
        ")\n",
        "sgd.fit(X_train_s, y_train)\n",
        "\n",
        "# 4) Evaluate accuracy\n",
        "def eval_acc(name, model):\n",
        "    pred = model.predict(X_test_s)\n",
        "    print(f\"{name:26s} test acc = {accuracy_score(y_test, pred):.4f}\")\n",
        "\n",
        "eval_acc(\"SVC(kernel='linear')\", svc)\n",
        "eval_acc(\"LinearSVC\", linearsvc)\n",
        "eval_acc(\"SGDClassifier(hinge)\", sgd)\n",
        "\n",
        "# 5) Compare the learned hyperplanes (direction of w)\n",
        "# Note: all trained on the SAME scaled data => w is directly comparable.\n",
        "w_svc, b_svc = svc.coef_.ravel(), float(svc.intercept_[0])\n",
        "w_lsv, b_lsv = linearsvc.coef_.ravel(), float(linearsvc.intercept_[0])\n",
        "w_sgd, b_sgd = sgd.coef_.ravel(), float(sgd.intercept_[0])\n",
        "\n",
        "def cos_sim(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
        "\n",
        "print(\"\\nCosine similarity between weight vectors (¬±1 means same direction):\")\n",
        "print(\"  SVC vs LinearSVC:\", cos_sim(w_svc, w_lsv))\n",
        "print(\"  SVC vs SGD      :\", cos_sim(w_svc, w_sgd))\n",
        "print(\"  LinearSVC vs SGD:\", cos_sim(w_lsv, w_sgd))\n",
        "\n",
        "print(\"\\nIntercepts (b):\")\n",
        "print(\"  SVC      :\", b_svc)\n",
        "print(\"  LinearSVC:\", b_lsv)\n",
        "print(\"  SGD      :\", b_sgd)\n",
        "\n",
        "print(\"\\n# Support vectors (SVC only):\", len(svc.support_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjIKzKGrCZeP",
        "outputId": "fa5783cc-c608-440c-f1b3-31e35748ed29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC(kernel='linear')       test acc = 0.9973\n",
            "LinearSVC                  test acc = 0.9973\n",
            "SGDClassifier(hinge)       test acc = 0.9973\n",
            "\n",
            "Cosine similarity between weight vectors (¬±1 means same direction):\n",
            "  SVC vs LinearSVC: 0.9869088995683594\n",
            "  SVC vs SGD      : 0.9757334343153079\n",
            "  LinearSVC vs SGD: 0.9921400725507815\n",
            "\n",
            "Intercepts (b):\n",
            "  SVC      : 0.722504011649743\n",
            "  LinearSVC: 0.3255123367277458\n",
            "  SGD      : 78.85635406986505\n",
            "\n",
            "# Support vectors (SVC only): 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9) SVM classifier on MNIST with One-vs-Rest (OvR) + quick tuning\n",
        "\n",
        "This does OvR explicitly (as requested). RBF SVM on full MNIST can be slow; the code tunes on a smaller subset first."
      ],
      "metadata": {
        "id": "unTzWgXCCcIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1) Load MNIST (70k x 784)\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, as_frame=False, return_X_y=True)\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "# Optional: scale pixels to [0,1] first (helps numerics)\n",
        "X = X / 255.0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Baseline: OvR Linear SVM (fast)\n",
        "linear_ovr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", OneVsRestClassifier(LinearSVC(C=1.0, max_iter=20000)))\n",
        "])\n",
        "linear_ovr.fit(X_train, y_train)\n",
        "pred = linear_ovr.predict(X_test)\n",
        "print(\"OvR LinearSVC test acc:\", accuracy_score(y_test, pred))\n",
        "\n",
        "# 3) RBF SVM (OvR) with tuning on a small subset\n",
        "# Subsample to speed up CV tuning\n",
        "rng = np.random.RandomState(42)\n",
        "subset_size = 12000\n",
        "idx = rng.choice(len(X_train), size=subset_size, replace=False)\n",
        "X_sub, y_sub = X_train[idx], y_train[idx]\n",
        "\n",
        "rbf_ovr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", OneVsRestClassifier(SVC(kernel=\"rbf\")))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"clf__estimator__C\": [1, 10, 100],\n",
        "    \"clf__estimator__gamma\": [0.01, 0.03, 0.1],\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(rbf_ovr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "gs.fit(X_sub, y_sub)\n",
        "\n",
        "print(\"\\nBest CV score:\", gs.best_score_)\n",
        "print(\"Best params  :\", gs.best_params_)\n",
        "\n",
        "# 4) Evaluate best RBF OvR model on the full test set\n",
        "best_rbf = gs.best_estimator_\n",
        "pred = best_rbf.predict(X_test)\n",
        "print(\"OvR RBF-SVC test acc:\", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "Q98OkIpwCfR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10) SVM regressor on California Housing (LinearSVR baseline + RBF SVR on subset)\n",
        "\n",
        "Full RBF SVR on the entire dataset can be heavy; this code trains RBF SVR on a subset (still the correct SVR method)."
      ],
      "metadata": {
        "id": "uBCmX4J4Ch5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVR, SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# RMSE helper compatible with newer sklearn\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "# 1) Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Baseline: LinearSVR (fast)\n",
        "linear_svr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svr\", LinearSVR(C=1.0, epsilon=0.1, random_state=42, max_iter=20000))\n",
        "])\n",
        "linear_svr.fit(X_train, y_train)\n",
        "pred = linear_svr.predict(X_test)\n",
        "print(\"LinearSVR RMSE:\", rmse(y_test, pred))\n",
        "\n",
        "# 3) RBF SVR (slower): tune on a subset for practicality\n",
        "rng = np.random.RandomState(42)\n",
        "subset_size = 12000\n",
        "idx = rng.choice(len(X_train), size=subset_size, replace=False)\n",
        "X_sub, y_sub = X_train[idx], y_train[idx]\n",
        "\n",
        "rbf_svr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svr\", SVR(kernel=\"rbf\"))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    \"svr__C\": [1, 10, 100],\n",
        "    \"svr__gamma\": [0.01, 0.1, 1.0],\n",
        "    \"svr__epsilon\": [0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(rbf_svr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
        "gs.fit(X_sub, y_sub)\n",
        "\n",
        "print(\"\\nBest CV RMSE (neg):\", gs.best_score_)\n",
        "print(\"Best params       :\", gs.best_params_)\n",
        "\n",
        "best_rbf_svr = gs.best_estimator_\n",
        "pred = best_rbf_svr.predict(X_test)\n",
        "print(\"RBF SVR RMSE:\", rmse(y_test, pred))"
      ],
      "metadata": {
        "id": "TsH8OXhxCk8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}