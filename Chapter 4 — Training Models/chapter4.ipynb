{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Training Models\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint References)\n",
        "\n",
        "From `4_Training Models.pdf`:\n",
        "\n",
        "| Topic | Keywords to Find |\n",
        "|---|---|\n",
        "| Linear Regression | `Linear Regression`, `MSE`, `cost function`, `theta` |\n",
        "| The Normal Equation | `Normal Equation`, `X^T X`, `inverse`, `pseudoinverse` |\n",
        "| Gradient Descent *(next)* | `Gradient Descent`, `learning rate`, `Batch Gradient Descent` |\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Linear Regression Is Trying to Do\n",
        "\n",
        "We have training data:\n",
        "* **Inputs (features):** $\\mathbf{x}$\n",
        "* **Targets (labels):** $y$\n",
        "\n",
        "**Goal:** learn a prediction rule that maps inputs to outputs.\n",
        "\n",
        "For one feature:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x$$\n",
        "\n",
        "* $\\theta_0$ = bias / intercept\n",
        "* $\\theta_1$ = slope / weight\n",
        "\n",
        "For many features, add a constant $x_0 = 1$ so the bias fits naturally:\n",
        "\n",
        "$$\\mathbf{x} = [1, x_1, x_2, \\dots, x_n]$$\n",
        "\n",
        "$$\\hat{y} = \\mathbf{x}^\\top \\boldsymbol{\\theta}$$\n",
        "\n",
        "> Same idea: **\"weighted sum of features + bias\"**\n",
        "\n",
        "---\n",
        "\n",
        "### 2) How We Decide What \"Best Parameters\" Means (The Cost Function)\n",
        "\n",
        "We need a score that measures **\"how wrong\"** our predictions are.\n",
        "\n",
        "The chapter uses **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
        "\n",
        "* $m$ = number of training examples\n",
        "* $(\\hat{y}^{(i)} - y^{(i)})$ = error (residual)\n",
        "* Squaring makes errors positive and **punishes large errors more**\n",
        "\n",
        "> üìù **Key idea:** Training linear regression = choose $\\boldsymbol{\\theta}$ that **minimizes MSE**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) The Vector/Matrix View (Why the Book Likes It)\n",
        "\n",
        "Stack all training examples into a matrix:\n",
        "\n",
        "* $\\mathbf{X}$ = design matrix (each row is one instance, including leading `1` for bias)\n",
        "* $\\mathbf{y}$ = vector of targets\n",
        "* $\\boldsymbol{\\theta}$ = parameter vector\n",
        "\n",
        "Predictions for **all instances at once**:\n",
        "\n",
        "$$\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\theta}$$\n",
        "\n",
        "> This makes formulas **compact** and computation **efficient**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Normal Equation (Direct Solution ‚Äî No Iteration)\n",
        "\n",
        "Instead of \"try values until it's good\", linear regression has a **direct minimizer**:\n",
        "\n",
        "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}$$\n",
        "\n",
        "**When it's nice:**\n",
        "* Simple, one-shot solution\n",
        "* Great when number of features isn't huge\n",
        "\n",
        "**Practical note ‚ö†Ô∏è:**\n",
        "* Sometimes $(\\mathbf{X}^\\top \\mathbf{X})$ isn't invertible (or is numerically unstable)\n",
        "* In practice we use a numerically stable method (pseudo-inverse / SVD) under the hood"
      ],
      "metadata": {
        "id": "IUDePGO6KAQ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHSYf5JzGYqE",
        "outputId": "3cc20089-4ace-410d-f9d4-227c6139aa4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta_best (Normal Equation): [4.21509616 2.77011339]\n",
            "sklearn intercept_, coef_: 4.215096157546747 2.7701133864384837\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Synthetic data: y = 4 + 3x + noise\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1)\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)\n",
        "\n",
        "# Normal Equation\n",
        "X_b = np.c_[np.ones((m, 1)), X]          # add x0 = 1 (bias feature)\n",
        "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "\n",
        "print(\"theta_best (Normal Equation):\", theta_best.ravel())\n",
        "\n",
        "# scikit-learn check\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X, y)\n",
        "print(\"sklearn intercept_, coef_:\", lin_reg.intercept_[0], lin_reg.coef_[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 2: Gradient Descent for Linear Regression (Batch GD)\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find subsection **\"Gradient Descent\"** ‚Üí **\"Batch Gradient Descent\"**\n",
        "\n",
        "Keywords: `Gradient Descent`, `Batch Gradient Descent`, `learning rate`, `eta`, `partial derivatives`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Why We Need Gradient Descent\n",
        "\n",
        "The Normal Equation is a direct solution, but it becomes **costly when the number of features is large**.\n",
        "\n",
        "> **Gradient Descent idea:** instead of solving in one shot, start with some $\\boldsymbol{\\theta}$ and repeatedly improve it by taking small steps **downhill** on the cost function.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) The \"Downhill\" Direction = The Gradient\n",
        "\n",
        "For linear regression, the cost function is MSE:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
        "\n",
        "The gradient tells us which direction **increases** the cost the most. To **decrease** the cost, we move in the **opposite direction**:\n",
        "\n",
        "$$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
        "\n",
        "* $\\eta$ = **learning rate** (step size)\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Batch Gradient Descent\n",
        "\n",
        "Batch GD uses the **entire training set** to compute the gradient at each step.\n",
        "\n",
        "In matrix form (with $X_b$ including the column of 1s):\n",
        "\n",
        "$$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) = \\frac{2}{m}X_b^T(X_b\\boldsymbol{\\theta} - \\mathbf{y})$$\n",
        "\n",
        "So the update becomes:\n",
        "\n",
        "$$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta\\frac{2}{m}X_b^T(X_b\\boldsymbol{\\theta} - \\mathbf{y})$$\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Learning Rate Intuition\n",
        "\n",
        "| Learning Rate | Effect |\n",
        "|---|---|\n",
        "| Too small | Training is **slow** ‚Äî many steps to converge |\n",
        "| Too large | **Overshoot** the minimum ‚Äî may diverge |\n",
        "| Just right | Smooth convergence to minimum |\n",
        "\n",
        "> üìù **Key idea:** Gradient Descent is not a one-shot formula. It is an **iterative improvement process** guided by the slope of the cost function."
      ],
      "metadata": {
        "id": "EdieuH0zLg7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Data: y = 4 + 3x + noise\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1)\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)\n",
        "\n",
        "X_b = np.c_[np.ones((m, 1)), X]  # add bias feature\n",
        "\n",
        "eta = 0.1\n",
        "n_iterations = 1000\n",
        "\n",
        "theta = np.random.randn(2, 1)  # random init\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = (2/m) * X_b.T @ (X_b @ theta - y)\n",
        "    theta = theta - eta * gradients\n",
        "\n",
        "print(\"theta (Batch GD):\", theta.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9L97NjZKGz_",
        "outputId": "83725bd4-af07-434f-e9e5-e109e8513dcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta (Batch GD): [4.21509616 2.77011339]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 3: Stochastic Gradient Descent (SGD)\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find subsections:\n",
        "* **\"Stochastic Gradient Descent\"**\n",
        "* **\"Mini-batch Gradient Descent\"**\n",
        "* **\"Learning Schedules\"**\n",
        "\n",
        "Keywords: `Stochastic`, `Mini-batch`, `learning schedule`, `simulated annealing`, `t0`, `t1`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Changes vs Batch Gradient Descent?\n",
        "\n",
        "* **Batch GD:** uses all $m$ instances to compute the gradient each step\n",
        "* **SGD:** uses just **1 instance** (picked randomly) per step\n",
        "\n",
        "So instead of:\n",
        "\n",
        "$$\\nabla J(\\boldsymbol{\\theta}) = \\frac{2}{m}X_b^T(X_b\\boldsymbol{\\theta} - \\mathbf{y})$$\n",
        "\n",
        "SGD approximates the gradient using a single training example $(\\mathbf{x}^{(i)}, y^{(i)})$:\n",
        "\n",
        "$$\\nabla J(\\boldsymbol{\\theta}) \\approx 2\\,\\mathbf{x}^{(i)}(\\mathbf{x}^{(i)\\top}\\boldsymbol{\\theta} - y^{(i)})$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Why SGD Is Useful\n",
        "\n",
        "| | Batch GD | SGD |\n",
        "|---|---|---|\n",
        "| Gradient quality | Exact | Noisy approximation |\n",
        "| Cost per step | Expensive ($m$ samples) | Cheap (1 sample) |\n",
        "| Scalability | Poor on huge datasets | Scales well |\n",
        "| Convergence path | Smooth | Jiggles around minimum |\n",
        "\n",
        "> üìù **Key idea:** SGD trades gradient accuracy for speed. It often reaches a \"pretty good\" solution much faster than Batch GD.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) The Learning Schedule (Very Important)\n",
        "\n",
        "Because SGD is noisy, we **decrease the learning rate over time**:\n",
        "* **Big steps early** ‚Üí fast progress\n",
        "* **Smaller steps later** ‚Üí stabilize around the minimum\n",
        "\n",
        "> The chapter relates this to **\"simulated annealing\"**: gradually reducing randomness so the system settles into a minimum.\n",
        "\n",
        "A common simple schedule:\n",
        "\n",
        "$$\\eta(t) = \\frac{t_0}{t + t_1}$$\n",
        "\n",
        "* $t$ grows with iterations\n",
        "* $\\eta(t)$ slowly decreases over time\n",
        "* $t_0$ and $t_1$ are schedule hyperparameters you tune\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Mini-Batch Gradient Descent (The Middle Ground)\n",
        "\n",
        "| Variant | Instances per step |\n",
        "|---|---|\n",
        "| Batch GD | All $m$ |\n",
        "| SGD | 1 |\n",
        "| **Mini-batch GD** | Small batch (e.g. 32, 64, 128) |\n",
        "\n",
        "Mini-batch GD is the **most used in practice** ‚Äî it gets:\n",
        "* More stable gradients than SGD\n",
        "* Much faster updates than full Batch GD\n",
        "* GPU efficiency (hardware loves batches)\n",
        "\n",
        "> üìù **Key idea:** When people say \"SGD\" in deep learning, they almost always mean **mini-batch SGD**."
      ],
      "metadata": {
        "id": "sn9I8NwnM-C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Data: y = 4 + 3x + noise\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1)\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)\n",
        "X_b = np.c_[np.ones((m, 1)), X]\n",
        "\n",
        "# Learning schedule: eta(t) = t0 / (t + t1)\n",
        "t0, t1 = 5, 50\n",
        "def learning_rate(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "n_epochs = 50\n",
        "theta = np.random.randn(2, 1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_epochs):\n",
        "    # shuffle indices each epoch\n",
        "    for i in np.random.permutation(m):\n",
        "        xi = X_b[i:i+1]\n",
        "        yi = y[i:i+1]\n",
        "        gradients = 2 * xi.T @ (xi @ theta - yi)\n",
        "        eta = learning_rate(t)\n",
        "        theta = theta - eta * gradients\n",
        "        t += 1\n",
        "\n",
        "print(\"theta (SGD):\", theta.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B91xw5cDM-a0",
        "outputId": "d338b2e9-f7af-4fde-b05f-120a63abb03b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta (SGD): [4.21443559 2.76905559]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 4: Mini-Batch Gradient Descent\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find subsection **\"Mini-Batch Gradient Descent\"**\n",
        "\n",
        "Keywords: `Mini-batch`, `batch size`, `Stochastic`, `Batch`, `GPU`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Definition\n",
        "\n",
        "Mini-batch GD is the **\"middle ground\"** between Batch GD and SGD:\n",
        "\n",
        "| Variant | Instances per step |\n",
        "|---|---|\n",
        "| Batch GD | All $m$ instances |\n",
        "| SGD | 1 instance |\n",
        "| **Mini-batch GD** | Small batch of size $b$ (e.g. 16, 32, 64, 128) |\n",
        "\n",
        "Each update uses only $b$ examples:\n",
        "\n",
        "$$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\cdot \\nabla_{\\boldsymbol{\\theta}}J_{batch}(\\boldsymbol{\\theta})$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Why It's Popular in Practice\n",
        "\n",
        "**Compared to SGD:**\n",
        "* Gradient is **less noisy** ‚Üí updates are steadier\n",
        "* Converges more smoothly (less random bouncing)\n",
        "\n",
        "**Compared to Batch GD:**\n",
        "* Each step is **cheaper** than using all $m$\n",
        "* Works very well with **vectorized operations** (GPUs/CPUs love matrix batches)\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Key Tradeoff: Batch Size $b$\n",
        "\n",
        "| Batch Size | Behavior |\n",
        "|---|---|\n",
        "| Small $b$ | Faster steps, noisier updates ‚Üí more like SGD |\n",
        "| Large $b$ | Smoother, more expensive steps ‚Üí more like Batch GD |\n",
        "\n",
        "> üìù **Key idea:** Mini-batch GD is the **default choice in modern deep learning**. When people say \"SGD\" in practice, they almost always mean mini-batch SGD with a learning schedule.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Comparison ‚Äî All Three Variants\n",
        "\n",
        "| | Batch GD | SGD | Mini-batch GD |\n",
        "|---|---|---|---|\n",
        "| Gradient quality | Exact | Very noisy | Moderately noisy |\n",
        "| Cost per step | High | Very low | Low |\n",
        "| Convergence path | Smooth | Erratic | Steady |\n",
        "| GPU efficiency | Poor | Poor | **Excellent** |\n",
        "| Used in practice | Rarely | Sometimes | **Most common** |"
      ],
      "metadata": {
        "id": "3FXJVjwVPg94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Data: y = 4 + 3x + noise\n",
        "m = 100\n",
        "X = 2 * np.random.rand(m, 1)\n",
        "y = 4 + 3 * X + np.random.randn(m, 1)\n",
        "X_b = np.c_[np.ones((m, 1)), X]\n",
        "\n",
        "# Learning schedule (same idea as SGD)\n",
        "t0, t1 = 5, 50\n",
        "def learning_rate(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "n_epochs = 200\n",
        "batch_size = 20\n",
        "\n",
        "theta = np.random.randn(2, 1)\n",
        "t = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    shuffled_idx = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_idx]\n",
        "    y_shuffled = y[shuffled_idx]\n",
        "\n",
        "    for start in range(0, m, batch_size):\n",
        "        end = start + batch_size\n",
        "        X_batch = X_b_shuffled[start:end]\n",
        "        y_batch = y_shuffled[start:end]\n",
        "\n",
        "        gradients = (2/len(X_batch)) * X_batch.T @ (X_batch @ theta - y_batch)\n",
        "        eta = learning_rate(t)\n",
        "        theta = theta - eta * gradients\n",
        "        t += 1\n",
        "\n",
        "print(\"theta (Mini-batch GD):\", theta.ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8tnqdffNDLD",
        "outputId": "8c078952-0252-4fe4-ed4b-ff3e71f07a81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta (Mini-batch GD): [4.20609049 2.77894893]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 5: Polynomial Regression\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find section **\"Polynomial Regression\"** (comes after the Gradient Descent family)\n",
        "\n",
        "Keywords: `Polynomial Regression`, `PolynomialFeatures`, `overfitting`, `degree`, `learning curves`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Motivation: Linear Models Can Be \"Too Straight\"\n",
        "\n",
        "Plain linear regression fits:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x$$\n",
        "\n",
        "That's a **straight line**. But many real relationships are **curved**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Key Trick: Add Nonlinear Features, Keep the Model Linear in Parameters\n",
        "\n",
        "Polynomial regression does **not** make the model nonlinear in $\\boldsymbol{\\theta}$. It makes the **features** nonlinear (powers of $x$), then uses linear regression on those features.\n",
        "\n",
        "**Example (degree 2):**\n",
        "* Create features: $x, x^2$\n",
        "* Fit:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2$$\n",
        "\n",
        "**For degree $d$:**\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\cdots + \\theta_d x^d$$\n",
        "\n",
        "> üìù **Key idea:** Polynomial regression is still **\"linear regression\"** ‚Äî it is linear with respect to the parameters $\\theta_0, \\dots, \\theta_d$. The nonlinearity is in the **features**, not the model.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Overfitting Risk Increases With Degree\n",
        "\n",
        "* Higher degree ‚Üí more flexible curve ‚Üí fits training data extremely well\n",
        "* But it may start **fitting noise** ‚Üí test error gets worse\n",
        "\n",
        "This is the classic **bias‚Äìvariance tradeoff**:\n",
        "\n",
        "| Degree | Bias | Variance | Result |\n",
        "|---|---|---|---|\n",
        "| Too low | High | Low | **Underfit** |\n",
        "| Just right | Balanced | Balanced | ‚úÖ Good generalization |\n",
        "| Too high | Low | High | **Overfit** |\n",
        "\n",
        "---\n",
        "\n",
        "### 4) How to Implement in Scikit-Learn\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "poly_reg = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"lin_reg\", LinearRegression())\n",
        "])\n",
        "\n",
        "poly_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "> `PolynomialFeatures` automatically creates all power combinations up to the chosen degree ‚Äî then `LinearRegression` fits on those expanded features as normal."
      ],
      "metadata": {
        "id": "vk8g3EcmQa6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Curved synthetic data\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3          # range ~[-3, 3]\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)  # quadratic + noise\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def fit_and_report(degree):\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lin_reg\", LinearRegression())\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    train_rmse = rmse(y_train, model.predict(X_train))\n",
        "    test_rmse  = rmse(y_test,  model.predict(X_test))\n",
        "    print(f\"degree={degree:2d} | train RMSE={train_rmse:.3f} | test RMSE={test_rmse:.3f}\")\n",
        "\n",
        "for d in [1, 2, 10]:\n",
        "    fit_and_report(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi_Xr66yQbzo",
        "outputId": "294f988a-5720-4e52-9d0d-ff5d39013d1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "degree= 1 | train RMSE=1.777 | test RMSE=1.592\n",
            "degree= 2 | train RMSE=0.903 | test RMSE=0.797\n",
            "degree=10 | train RMSE=0.878 | test RMSE=0.813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 6: Learning Curves\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find section **\"Learning Curves\"**\n",
        "\n",
        "Keywords: `Learning Curves`, `underfitting`, `overfitting`, `bias`, `variance`, `training error`, `validation error`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What a Learning Curve Is\n",
        "\n",
        "A learning curve plots model **error vs. amount of training data**.\n",
        "\n",
        "We track two curves:\n",
        "* **Training error** ‚Äî error on the training subset used to fit the model\n",
        "* **Validation error** ‚Äî error on held-out data (or CV folds)\n",
        "\n",
        "As training size increases:\n",
        "* Training error usually **goes up** (harder to fit more points perfectly)\n",
        "* Validation error usually **goes down** (more data helps generalization)\n",
        "\n",
        "> üìù **Key idea:** The *gap* between the two curves, and their *absolute levels*, tell you whether you're underfitting or overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Pattern 1 ‚Äî Underfitting (High Bias)\n",
        "\n",
        "**Signs:**\n",
        "* Training error is **high**\n",
        "* Validation error is **high**\n",
        "* They are **close together** (small gap)\n",
        "\n",
        "**Interpretation:**\n",
        "> The model is too simple to capture the pattern. Adding more data doesn't help much.\n",
        "\n",
        "**Typical fixes:**\n",
        "* Use a more expressive model (e.g., higher polynomial degree)\n",
        "* Add better features\n",
        "* Reduce regularization (if applied)\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Pattern 2 ‚Äî Overfitting (High Variance)\n",
        "\n",
        "**Signs:**\n",
        "* Training error is **low**\n",
        "* Validation error is **significantly higher**\n",
        "* There is a **big gap** between them\n",
        "\n",
        "**Interpretation:**\n",
        "> The model is fitting noise ‚Äî too flexible. More data often reduces the gap but may not be enough alone.\n",
        "\n",
        "**Typical fixes:**\n",
        "* Simplify the model (lower degree)\n",
        "* Add regularization (Ridge / Lasso)\n",
        "* Get more training data\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Comparison\n",
        "\n",
        "| | Underfitting | Overfitting |\n",
        "|---|---|---|\n",
        "| Training error | High | Low |\n",
        "| Validation error | High | High |\n",
        "| Gap between curves | Small | **Large** |\n",
        "| Bias | High | Low |\n",
        "| Variance | Low | High |\n",
        "| Fix | More complexity | Less complexity / regularization |\n",
        "\n",
        "---\n",
        "\n",
        "### 5) The Bias‚ÄìVariance Tradeoff (Unified View)\n",
        "\n",
        "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
        "\n",
        "* **Bias** = error from wrong assumptions (model too simple)\n",
        "* **Variance** = error from sensitivity to training data (model too complex)\n",
        "* **Irreducible noise** = inherent noise in the data ‚Äî cannot be reduced\n",
        "\n",
        "> üìù **Key idea:** Learning curves are your **diagnostic tool**. Before changing a model, plot learning curves to know *which direction* to move."
      ],
      "metadata": {
        "id": "RpcVNTC-Rtqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Same curved synthetic data idea as before\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + np.random.randn(m, 1)).ravel()\n",
        "\n",
        "def report_learning_curve(degree):\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lin_reg\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "    train_sizes, train_scores, val_scores = learning_curve(\n",
        "        model, X, y,\n",
        "        cv=5,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    train_rmse = np.sqrt(-train_scores)\n",
        "    val_rmse = np.sqrt(-val_scores)\n",
        "\n",
        "    print(f\"\\nDegree = {degree}\")\n",
        "    for s, tr, va in zip(train_sizes, train_rmse.mean(axis=1), val_rmse.mean(axis=1)):\n",
        "        print(f\"  m={int(s):3d} | train RMSE={tr:.3f} | val RMSE={va:.3f}\")\n",
        "\n",
        "report_learning_curve(1)\n",
        "report_learning_curve(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLN9iF7CRuKR",
        "outputId": "806b7d34-0b5c-4b4f-fd93-d7374d59d675"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Degree = 1\n",
            "  m=  8 | train RMSE=1.395 | val RMSE=1.899\n",
            "  m= 26 | train RMSE=1.764 | val RMSE=1.800\n",
            "  m= 44 | train RMSE=1.765 | val RMSE=1.756\n",
            "  m= 62 | train RMSE=1.716 | val RMSE=1.745\n",
            "  m= 80 | train RMSE=1.738 | val RMSE=1.745\n",
            "\n",
            "Degree = 10\n",
            "  m=  8 | train RMSE=0.000 | val RMSE=220.479\n",
            "  m= 26 | train RMSE=0.748 | val RMSE=1.213\n",
            "  m= 44 | train RMSE=0.829 | val RMSE=1.065\n",
            "  m= 62 | train RMSE=0.827 | val RMSE=1.060\n",
            "  m= 80 | train RMSE=0.842 | val RMSE=1.029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 7: Regularized Linear Models ‚Äî Ridge Regression\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Why Degree 10 Screams \"High Variance\"\n",
        "\n",
        "* At $m = 8$, train RMSE = `0.000` ‚Üí the model **memorizes** those few points (a wiggly curve passes through all of them)\n",
        "* But val RMSE = `220.479` ‚Üí generalizes horribly ‚Äî the curve swings wildly between points, so predictions **explode** for some validation $x$ values\n",
        "* As $m$ increases (26, 44, 62, 80), validation RMSE drops toward ~1.0 ‚Äî more data **constrains** the curve, reducing the worst overfitting\n",
        "\n",
        "> Very low train error + much higher validation error (big gap) ‚áí **high variance / overfitting** ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find sections **\"Regularized Linear Models\"** ‚Üí **\"Ridge Regression\"**\n",
        "\n",
        "Keywords: `Regularized Linear Models`, `Ridge`, `L2`, `alpha`, `||Œ∏||^2`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Why Regularization?\n",
        "\n",
        "When a model is too flexible (e.g., high-degree polynomial features), it gets **high variance** (overfits). Regularization combats this by **discouraging large weights**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Ridge Regression = Linear Regression + L2 Penalty\n",
        "\n",
        "Ridge modifies the cost function by adding a penalty on the **size of the weights**:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{j=1}^{n}\\theta_j^2$$\n",
        "\n",
        "* $\\sum \\theta_j^2$ = the **L2 norm squared**\n",
        "* $\\alpha \\geq 0$ = controls how strong the penalty is\n",
        "* The bias term $\\theta_0$ is **not penalized**\n",
        "\n",
        "---\n",
        "\n",
        "### 3) What $\\alpha$ Does (Key Intuition)\n",
        "\n",
        "| $\\alpha$ value | Effect |\n",
        "|---|---|\n",
        "| $\\alpha = 0$ | Plain Linear Regression ‚Äî no regularization |\n",
        "| Small $\\alpha$ | Slight shrinkage ‚Äî close to unregularized |\n",
        "| Large $\\alpha$ | Weights shrink toward 0 ‚Üí simpler model ‚Üí lower variance, higher bias |\n",
        "\n",
        "> üìù **Key idea:** Ridge does **not** eliminate features ‚Äî it **shrinks** all weights. It trades a small increase in bias for a potentially large reduction in variance, improving test performance when the unregularized model overfits.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Bias‚ÄìVariance View of Ridge\n",
        "\n",
        "$$\\text{Total Error} = \\underbrace{\\text{Bias}^2}_{\\uparrow \\text{ with } \\alpha} + \\underbrace{\\text{Variance}}_{\\downarrow \\text{ with } \\alpha} + \\text{Irreducible Noise}$$\n",
        "\n",
        "Ridge shifts the tradeoff:\n",
        "* Unregularized overfit model ‚Üí high variance dominates\n",
        "* Adding $\\alpha$ ‚Üí variance drops, bias rises slightly ‚Üí **net improvement** in test error"
      ],
      "metadata": {
        "id": "cyC2AyD8TPdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + np.random.randn(m, 1)).ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def eval_model(model, name):\n",
        "    model.fit(X_train, y_train)\n",
        "    tr = rmse(y_train, model.predict(X_train))\n",
        "    te = rmse(y_test, model.predict(X_test))\n",
        "    print(f\"{name:>18s} | train RMSE={tr:.3f} | test RMSE={te:.3f}\")\n",
        "\n",
        "degree = 10\n",
        "\n",
        "plain = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lin\", LinearRegression())\n",
        "])\n",
        "\n",
        "ridge_1 = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n",
        "])\n",
        "\n",
        "ridge_10 = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ridge\", Ridge(alpha=10.0, random_state=42))\n",
        "])\n",
        "\n",
        "eval_model(plain,   \"LinearRegression\")\n",
        "eval_model(ridge_1, \"Ridge alpha=1\")\n",
        "eval_model(ridge_10,\"Ridge alpha=10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0RLEYhvTQH0",
        "outputId": "5085467a-8475-42a8-952d-2fc42f54f6fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LinearRegression | train RMSE=0.878 | test RMSE=0.813\n",
            "     Ridge alpha=1 | train RMSE=0.893 | test RMSE=0.798\n",
            "    Ridge alpha=10 | train RMSE=0.968 | test RMSE=0.818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 8: Lasso Regression (L1 Regularization)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Ridge Œ± Limits (Quick Reference)\n",
        "\n",
        "* As $\\alpha \\to 0$: Ridge ‚Üí ordinary linear regression (almost no regularization)\n",
        "* As $\\alpha \\to \\infty$: weights $\\theta_1, \\theta_2, \\dots$ are pushed toward 0 ‚Üí very simple model, low variance, high bias\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, under **\"Regularized Linear Models\"** ‚Üí find **\"Lasso Regression\"**\n",
        "\n",
        "Keywords: `Lasso`, `L1`, `Least Absolute Shrinkage`, `feature selection`, `||Œ∏||_1`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Lasso Is\n",
        "\n",
        "Lasso regression is linear regression with an **L1 penalty** added to the cost:\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{j=1}^{n}|\\theta_j|$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Ridge vs Lasso ‚Äî The Key Behavioral Difference\n",
        "\n",
        "| | Ridge (L2) | Lasso (L1) |\n",
        "|---|---|---|\n",
        "| Penalty term | $\\sum \\theta_j^2$ | $\\sum \\|\\theta_j\\|$ |\n",
        "| Effect on weights | Shrinks smoothly toward 0 | Can shrink **exactly to 0** |\n",
        "| Feature selection | ‚ùå No | ‚úÖ Yes |\n",
        "| When to prefer | Many small useful features | Sparse model ‚Äî few features matter |\n",
        "\n",
        "> üìù **Key idea:** Lasso performs **automatic feature selection** by driving some coefficients to exactly 0. Ridge never fully eliminates features ‚Äî it only shrinks them.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) What $\\alpha$ Does in Lasso\n",
        "\n",
        "| $\\alpha$ value | Effect |\n",
        "|---|---|\n",
        "| $\\alpha \\to 0$ | Close to ordinary linear regression |\n",
        "| Larger $\\alpha$ | More coefficients go to exactly 0 ‚Üí sparser model ‚Üí bias ‚Üë, variance ‚Üì |\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Bias‚ÄìVariance View of Lasso\n",
        "\n",
        "$$\\text{Total Error} = \\underbrace{\\text{Bias}^2}_{\\uparrow \\text{ with } \\alpha} + \\underbrace{\\text{Variance}}_{\\downarrow \\text{ with } \\alpha} + \\text{Irreducible Noise}$$\n",
        "\n",
        "> Lasso shifts the same bias‚Äìvariance tradeoff as Ridge ‚Äî but its route is **sparsity**: it explicitly removes features rather than just shrinking them."
      ],
      "metadata": {
        "id": "pwZnhnuDWFZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + np.random.randn(m, 1)).ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "degree = 10\n",
        "\n",
        "plain = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"lin\", LinearRegression())\n",
        "])\n",
        "\n",
        "ridge = Pipeline([\n",
        "    (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"ridge\", Ridge(alpha=1.0))\n",
        "])\n",
        "\n",
        "def eval_lasso(alpha):\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lasso\", Lasso(alpha=alpha, max_iter=20000))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    te = rmse(y_test, model.predict(X_test))\n",
        "\n",
        "    # Count non-zero coefficients (after poly+scaling)\n",
        "    coefs = model.named_steps[\"lasso\"].coef_\n",
        "    nnz = np.sum(np.abs(coefs) > 1e-8)\n",
        "    return te, nnz\n",
        "\n",
        "plain.fit(X_train, y_train)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "print(f\"LinearRegression | test RMSE={rmse(y_test, plain.predict(X_test)):.3f}\")\n",
        "print(f\"Ridge(alpha=1)   | test RMSE={rmse(y_test, ridge.predict(X_test)):.3f}\")\n",
        "\n",
        "for a in [0.001, 0.01, 0.1]:\n",
        "    te, nnz = eval_lasso(a)\n",
        "    print(f\"Lasso(alpha={a}) | test RMSE={te:.3f} | nonzero coefs={nnz}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XACienGyWFxY",
        "outputId": "885da89e-8397-49c1-981d-bc8aeed82bda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression | test RMSE=0.813\n",
            "Ridge(alpha=1)   | test RMSE=0.798\n",
            "Lasso(alpha=0.001) | test RMSE=0.832 | nonzero coefs=6\n",
            "Lasso(alpha=0.01) | test RMSE=0.805 | nonzero coefs=5\n",
            "Lasso(alpha=0.1) | test RMSE=0.797 | nonzero coefs=2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 9: Elastic Net (L1 + L2 Together)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why L1 Creates Sparsity (Quick Reference)\n",
        "\n",
        "* **L1 penalty** has a \"diamond\" shape with sharp corners ‚Üí the optimum often lands on an axis ‚Üí some $\\theta_j = 0$ (sparse)\n",
        "* **L2 penalty** has a smooth \"circle\" ‚Üí shrinks weights but rarely hits exactly 0\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, under **\"Regularized Linear Models\"** ‚Üí find **\"Elastic Net\"**\n",
        "\n",
        "Keywords: `Elastic Net`, `l1_ratio`, `mix`, `L1`, `L2`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Elastic Net Is\n",
        "\n",
        "Elastic Net is a **mix of Ridge and Lasso** ‚Äî it adds both penalties:\n",
        "* **L1 term** ‚Üí encourages sparsity (feature selection)\n",
        "* **L2 term** ‚Üí encourages small weights and stability\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha\\Big(r\\sum|\\theta_j| + \\frac{1-r}{2}\\sum \\theta_j^2\\Big)$$\n",
        "\n",
        "* $\\alpha$ = controls overall regularization strength\n",
        "* $r$ (`l1_ratio`) = controls the mix:\n",
        "\n",
        "| $r$ value | Behavior |\n",
        "|---|---|\n",
        "| $r = 1$ | Pure Lasso |\n",
        "| $r = 0$ | Pure Ridge |\n",
        "| $0 < r < 1$ | Elastic Net (mix of both) |\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Why Elastic Net Can Be Better Than Pure Lasso\n",
        "\n",
        "Elastic Net is especially useful when:\n",
        "* Features are **strongly correlated** ‚Äî Lasso may pick one and drop the rest arbitrarily; Elastic Net handles groups of correlated features more stably\n",
        "* You want **some sparsity but also stability** ‚Äî L2 prevents erratic weight elimination\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Full Regularization Comparison\n",
        "\n",
        "| Method | Penalty | Sparsity | Stability | Best When |\n",
        "|---|---|---|---|---|\n",
        "| Ridge | L2 only | ‚ùå No | ‚úÖ High | Many small useful features |\n",
        "| Lasso | L1 only | ‚úÖ Yes | ‚ö†Ô∏è Can be erratic | Few features truly matter |\n",
        "| **Elastic Net** | L1 + L2 | ‚úÖ Yes | ‚úÖ High | Correlated features / default safe choice |\n",
        "\n",
        "> üìù **Key idea:** When in doubt between Ridge and Lasso, **Elastic Net is the safer default** ‚Äî it inherits the best of both."
      ],
      "metadata": {
        "id": "2n6gy_QvWK-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + np.random.randn(m, 1)).ravel()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "degree = 10\n",
        "\n",
        "def eval_elastic(alpha, l1_ratio):\n",
        "    model = Pipeline([\n",
        "        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"enet\", ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=50000, random_state=42))\n",
        "    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    te = rmse(y_test, model.predict(X_test))\n",
        "    coefs = model.named_steps[\"enet\"].coef_\n",
        "    nnz = np.sum(np.abs(coefs) > 1e-8)\n",
        "    print(f\"ElasticNet(alpha={alpha}, l1_ratio={l1_ratio}) | test RMSE={te:.3f} | nonzero coefs={nnz}\")\n",
        "\n",
        "for l1r in [0.2, 0.5, 0.8]:\n",
        "    eval_elastic(alpha=0.1, l1_ratio=l1r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JQFquzfXo8M",
        "outputId": "17e675ba-9fa0-4cdb-c449-23ceb73dbef8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ElasticNet(alpha=0.1, l1_ratio=0.2) | test RMSE=0.811 | nonzero coefs=5\n",
            "ElasticNet(alpha=0.1, l1_ratio=0.5) | test RMSE=0.804 | nonzero coefs=4\n",
            "ElasticNet(alpha=0.1, l1_ratio=0.8) | test RMSE=0.798 | nonzero coefs=2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 10: Early Stopping\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, inside or after **\"Regularized Linear Models\"** ‚Üí find **\"Early Stopping\"**\n",
        "\n",
        "Keywords: `Early Stopping`, `validation`, `overfitting`, `stop`, `best model`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Early Stopping Means\n",
        "\n",
        "When you train an iterative model (GD / SGD / mini-batch), you typically run for many epochs.\n",
        "\n",
        "**Early stopping rule:**\n",
        "* Keep a **validation set**\n",
        "* Track **validation error** during training\n",
        "* **Stop training** when validation error stops improving (or starts getting worse)\n",
        "\n",
        "This prevents the model from entering the **\"overfitting zone\"**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Why It Acts Like Regularization\n",
        "\n",
        "Overfitting often happens because the model keeps **adapting to noise** as training continues.\n",
        "\n",
        "Early stopping reduces overfitting by **limiting training time** ‚Äî which effectively limits how complex the fitted solution becomes.\n",
        "\n",
        "> üìù **Key idea:** Early stopping = **regularization by stopping before the model overfits**. No penalty term needed ‚Äî time itself is the constraint.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Practical Implementation Detail (Important)\n",
        "\n",
        "We don't just \"stop at the last epoch\" ‚Äî we **save the best parameters seen on validation** (best epoch), not the final epoch.\n",
        "```python\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Track best model during training\n",
        "best_val_error = float(\"inf\")\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.partial_fit(X_train, y_train)\n",
        "    val_error = mean_squared_error(y_val, model.predict(X_val))\n",
        "\n",
        "    if val_error < best_val_error:\n",
        "        best_val_error = val_error\n",
        "        best_model = clone(model)  # save best weights\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Full Regularization Methods ‚Äî Chapter 4 Summary\n",
        "\n",
        "| Method | How It Regularizes | Sparsity |\n",
        "|---|---|---|\n",
        "| Ridge | L2 penalty on weights | ‚ùå |\n",
        "| Lasso | L1 penalty on weights | ‚úÖ |\n",
        "| Elastic Net | L1 + L2 penalty | ‚úÖ |\n",
        "| **Early Stopping** | Limits training time / complexity | ‚ùå |\n",
        "\n",
        "> All four methods fight the same enemy: **overfitting**. They just attack it differently."
      ],
      "metadata": {
        "id": "6QAx-2oMZBMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Noisy curved data (makes overfitting easier to observe)\n",
        "m = 200\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + 1.5*np.random.randn(m, 1)).ravel()\n",
        "\n",
        "# Split: train / val / test\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)  # 0.25 of 0.8 = 0.2\n",
        "\n",
        "degree = 30\n",
        "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_p = scaler.fit_transform(poly.fit_transform(X_train))\n",
        "X_val_p   = scaler.transform(poly.transform(X_val))\n",
        "X_test_p  = scaler.transform(poly.transform(X_test))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "sgd = SGDRegressor(\n",
        "    random_state=42,\n",
        "    max_iter=1,      # we control epochs manually\n",
        "    tol=None,\n",
        "    learning_rate=\"invscaling\",\n",
        "    eta0=0.1,\n",
        "    power_t=0.25,\n",
        "    penalty=\"l2\",\n",
        "    alpha=1e-4\n",
        ")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_coef = None\n",
        "best_intercept = None\n",
        "\n",
        "patience = 20\n",
        "patience_left = patience\n",
        "\n",
        "n_epochs = 500\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    sgd.partial_fit(X_train_p, y_train)\n",
        "\n",
        "    val_rmse = rmse(y_val, sgd.predict(X_val_p))\n",
        "\n",
        "    if val_rmse < best_val - 1e-6:\n",
        "        best_val = val_rmse\n",
        "        best_epoch = epoch\n",
        "        best_coef = sgd.coef_.copy()\n",
        "        best_intercept = sgd.intercept_.copy()\n",
        "        patience_left = patience\n",
        "    else:\n",
        "        patience_left -= 1\n",
        "        if patience_left == 0:\n",
        "            break\n",
        "\n",
        "# Restore best model (this is the key part of early stopping)\n",
        "sgd.coef_ = best_coef\n",
        "sgd.intercept_ = best_intercept\n",
        "\n",
        "test_rmse = rmse(y_test, sgd.predict(X_test_p))\n",
        "\n",
        "print(f\"degree={degree}\")\n",
        "print(f\"stopped_epoch={epoch}, best_epoch={best_epoch}\")\n",
        "print(f\"best_val_RMSE={best_val:.3f}\")\n",
        "print(f\"test_RMSE={test_rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyNTNnhqY6a-",
        "outputId": "6b27d819-1533-4b6e-c2e1-27c4e2ad57ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "degree=30\n",
            "stopped_epoch=20, best_epoch=0\n",
            "best_val_RMSE=3128.278\n",
            "test_RMSE=4875.840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why We Keep the Best Epoch (Not the Final Epoch)\n",
        "\n",
        "When you train iteratively (SGD / mini-batch / GD), two things happen over time:\n",
        "\n",
        "1. **Training error** usually keeps improving ‚Üí the model fits the training set better and better\n",
        "2. **Validation error** improves at first, then often **gets worse** once the model starts overfitting (fitting noise / idiosyncrasies in the training set)\n",
        "\n",
        "> The best generalization happens at the epoch where **validation error is minimal** ‚Äî not the last epoch.\n",
        "\n",
        "---\n",
        "\n",
        "## The Key Point\n",
        "\n",
        "Early stopping works like this:\n",
        "\n",
        "* Monitor validation RMSE each epoch\n",
        "* When it stops improving, don't stop instantly (validation can be noisy) ‚Üí **wait a bit** (patience)\n",
        "* That means the final epoch is often **after** the best epoch\n",
        "* Therefore: **save and restore weights from the best validation epoch**, not the last epoch you happened to run\n",
        "\n",
        "> üìù **In one sentence:** We keep the best validation model because the last trained weights may already be **\"past the sweet spot\"** ‚Äî starting to overfit or drift.\n",
        "\n",
        "---\n",
        "\n",
        "## Why `best_epoch = 0` and Huge RMSE Happens\n",
        "\n",
        "If your output shows `best_epoch=0` and validation/test RMSE in the thousands, this usually means:\n",
        "\n",
        "* Training is **numerically unstable / diverging** ‚Äî steps are too aggressive for this feature space\n",
        "* With **degree=30 polynomial features**, SGD becomes extremely sensitive\n",
        "* The very first epoch already overshoots, so epoch 0 (before any update) looks like the \"best\"\n",
        "\n",
        "**Fixes:**\n",
        "* Reduce polynomial degree (degree=30 is extreme)\n",
        "* Lower the learning rate (`eta0`)\n",
        "* Add feature scaling (`StandardScaler`) before fitting ‚Äî SGD is very sensitive to feature scale\n",
        "\n",
        "> üìù **Key insight:** Early stopping can only save you if training is stable enough to improve in the first place. Diverging training means the learning rate or feature scale needs fixing first."
      ],
      "metadata": {
        "id": "71KhvAqzb-Gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "m = 200\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = (0.5 * X**2 + X + 2 + 1.5*np.random.randn(m, 1)).ravel()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
        "\n",
        "degree = 20  # lowered from 30\n",
        "poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_p = scaler.fit_transform(poly.fit_transform(X_train))\n",
        "X_val_p   = scaler.transform(poly.transform(X_val))\n",
        "X_test_p  = scaler.transform(poly.transform(X_test))\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "sgd = SGDRegressor(\n",
        "    random_state=42,\n",
        "    max_iter=1,\n",
        "    tol=None,\n",
        "    learning_rate=\"invscaling\",\n",
        "    eta0=0.01,      # lowered from 0.1\n",
        "    power_t=0.5,\n",
        "    penalty=\"l2\",\n",
        "    alpha=1e-3,     # stronger regularization\n",
        "    average=True    # stabilizes SGD a lot\n",
        ")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_coef = None\n",
        "best_intercept = None\n",
        "\n",
        "patience = 20\n",
        "patience_left = patience\n",
        "\n",
        "n_epochs = 500\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    sgd.partial_fit(X_train_p, y_train)\n",
        "    val_rmse = rmse(y_val, sgd.predict(X_val_p))\n",
        "\n",
        "    if val_rmse < best_val - 1e-6:\n",
        "        best_val = val_rmse\n",
        "        best_epoch = epoch\n",
        "        best_coef = sgd.coef_.copy()\n",
        "        best_intercept = sgd.intercept_.copy()\n",
        "        patience_left = patience\n",
        "    else:\n",
        "        patience_left -= 1\n",
        "        if patience_left == 0:\n",
        "            break\n",
        "\n",
        "sgd.coef_ = best_coef\n",
        "sgd.intercept_ = best_intercept\n",
        "\n",
        "test_rmse = rmse(y_test, sgd.predict(X_test_p))\n",
        "\n",
        "print(f\"degree={degree}\")\n",
        "print(f\"stopped_epoch={epoch}, best_epoch={best_epoch}\")\n",
        "print(f\"best_val_RMSE={best_val:.3f}\")\n",
        "print(f\"test_RMSE={test_rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqGxfmqEZWVm",
        "outputId": "3d3bf843-aeb3-4666-b8d4-86ae8fae5d45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "degree=20\n",
            "stopped_epoch=499, best_epoch=499\n",
            "best_val_RMSE=1.508\n",
            "test_RMSE=1.652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 11: Logistic Regression (Binary Classification)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Early Stopping Patience ‚Äî Small Fix\n",
        "\n",
        "Patience is **not** about escaping local minima (for linear models the objective is convex ‚Äî no local minima issue).\n",
        "\n",
        "It's about **noise in the validation metric**: validation RMSE can wiggle up/down epoch to epoch (especially with SGD / shuffling). We wait `patience` epochs to confirm improvement really stopped ‚Äî not just a random blip.\n",
        "\n",
        "> `stopped_epoch=499, best_epoch=499` ‚Üí validation kept improving the whole run, so early stopping never triggered. ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find section **\"Logistic Regression\"** and subsections:\n",
        "\n",
        "| Subsection | Keywords |\n",
        "|---|---|\n",
        "| Intro + probability model | `Logistic Regression`, `sigmoid`, `logistic function` |\n",
        "| Decision boundary | `decision boundary`, `predict_proba`, `0.5` |\n",
        "| Cost function | `log loss`, `cross entropy`, `cost function` |\n",
        "| Multiclass extension | `Softmax Regression`, `multinomial` |\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) What Logistic Regression Is\n",
        "\n",
        "Despite the name, logistic regression is a **classifier**.\n",
        "\n",
        "It starts like linear regression ‚Äî compute a linear score:\n",
        "\n",
        "$$z = \\boldsymbol{\\theta}^\\top \\mathbf{x}$$\n",
        "\n",
        "Then converts that score into a **probability** using the sigmoid:\n",
        "\n",
        "$$\\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "$$\\hat{p} = P(y = 1 \\mid \\mathbf{x})$$\n",
        "\n",
        "> üìù **Key idea:** Logistic regression doesn't predict a value ‚Äî it predicts a **probability**, then thresholds it into a class.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Prediction Rule (Thresholding)\n",
        "\n",
        "$$\\hat{y} = \\begin{cases} 1 & \\text{if } \\hat{p} \\geq 0.5 \\\\ 0 & \\text{if } \\hat{p} < 0.5 \\end{cases}$$\n",
        "\n",
        "Because $\\sigma(z) \\geq 0.5 \\iff z \\geq 0$, the decision boundary is:\n",
        "\n",
        "$$\\boldsymbol{\\theta}^\\top \\mathbf{x} = 0$$\n",
        "\n",
        "> The boundary is **linear in feature space** ‚Äî a line in 2D, a plane in 3D, a hyperplane in higher dims.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) How It's Trained (Log Loss / Cross-Entropy)\n",
        "\n",
        "Instead of MSE, logistic regression minimizes **log loss (cross-entropy)**:\n",
        "\n",
        "* If $y = 1$: loss $= -\\log(\\hat{p})$\n",
        "* If $y = 0$: loss $= -\\log(1 - \\hat{p})$\n",
        "\n",
        "**Intuition:**\n",
        "* If $y=1$ and $\\hat{p} \\to 1$ ‚Üí loss $\\to 0$ ‚úÖ\n",
        "* If $y=1$ and $\\hat{p} \\to 0$ ‚Üí loss $\\to \\infty$ ‚ùå (heavily penalized)\n",
        "\n",
        "Over all training data, we minimize the **average log loss** (often + regularization).\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Regularization (Tie-in to Ridge/Lasso)\n",
        "\n",
        "In scikit-learn, logistic regression is **regularized by default** (commonly L2).\n",
        "\n",
        "| Parameter | Effect |\n",
        "|---|---|\n",
        "| `C` (sklearn) | $C = \\frac{1}{\\alpha}$ ‚Äî **smaller C = stronger regularization** |\n",
        "| `penalty='l2'` | Ridge-style ‚Äî shrinks weights |\n",
        "| `penalty='l1'` | Lasso-style ‚Äî can zero out weights |\n",
        "\n",
        "> üìù **Key idea:** The same bias‚Äìvariance tradeoff from Ridge/Lasso applies here. Regularization prevents logistic regression from overfitting when features are many or correlated."
      ],
      "metadata": {
        "id": "P4bET-pZb_KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data  # 4 features\n",
        "y = (iris.target == 2).astype(int)  # 1 if Virginica else 0\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg\", LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "proba = model.predict_proba(X_test)[:5]  # first 5 probability rows\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"First 5 predicted probabilities [P(class0), P(class1)]:\\n\", np.round(proba, 4))\n",
        "\n",
        "# optional: show learned parameters\n",
        "lr = model.named_steps[\"log_reg\"]\n",
        "print(\"Intercept:\", np.round(lr.intercept_, 4))\n",
        "print(\"Coefficients:\", np.round(lr.coef_, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUBQhuICcpyF",
        "outputId": "b42d5e34-3064-4d25-a641-4ec767baf44c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion matrix:\n",
            " [[20  0]\n",
            " [ 0 10]]\n",
            "First 5 predicted probabilities [P(class0), P(class1)]:\n",
            " [[0.791  0.209 ]\n",
            " [0.4184 0.5816]\n",
            " [0.7159 0.2841]\n",
            " [0.9461 0.0539]\n",
            " [1.     0.    ]]\n",
            "Intercept: [-3.5173]\n",
            "Coefficients: [[ 0.2788 -0.4579  2.1206  2.8669]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Topic 12: Softmax Regression (Multiclass Logistic Regression)\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint)\n",
        "\n",
        "From `4_Training Models.pdf`, find **\"Softmax Regression\"** (near end of Logistic Regression section)\n",
        "\n",
        "Keywords: `Softmax`, `multinomial`, `cross entropy`, `log loss`, `K classes`\n",
        "\n",
        "---\n",
        "\n",
        "## B) Explanation\n",
        "\n",
        "### 1) Why We Need Softmax\n",
        "\n",
        "Binary logistic regression models $P(y=1 \\mid \\mathbf{x})$.\n",
        "\n",
        "But if $y$ can be $0, 1, 2, \\dots, K-1$, we want a **probability for each class** ‚Äî and they must sum to 1.\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Model: One Linear Score Per Class\n",
        "\n",
        "For each class $k$, compute a score (logit):\n",
        "\n",
        "$$s_k(\\mathbf{x}) = \\boldsymbol{\\theta}_k^\\top \\mathbf{x}$$\n",
        "\n",
        "Collect into a vector of scores $\\mathbf{s}(\\mathbf{x})$.\n",
        "\n",
        "> This is the same idea as OvR from Chapter 3 ‚Äî one scorer per class ‚Äî but now trained **jointly** instead of independently.\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Convert Scores to Probabilities With Softmax\n",
        "\n",
        "$$\\hat{p}_k = P(y=k \\mid \\mathbf{x}) = \\frac{e^{s_k(\\mathbf{x})}}{\\sum_{j=0}^{K-1} e^{s_j(\\mathbf{x})}}$$\n",
        "\n",
        "**Properties:**\n",
        "* All $\\hat{p}_k \\geq 0$\n",
        "* $\\sum_k \\hat{p}_k = 1$ (valid probability distribution)\n",
        "* The largest score gets the largest probability\n",
        "\n",
        "**Prediction rule:**\n",
        "\n",
        "$$\\hat{y} = \\arg\\max_k \\hat{p}_k = \\arg\\max_k s_k(\\mathbf{x})$$\n",
        "\n",
        "> Since Softmax is monotonic in the scores, argmax of probabilities = argmax of raw scores.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Training Objective: Cross-Entropy (Multiclass Log Loss)\n",
        "\n",
        "For one training example, the loss is:\n",
        "\n",
        "$$-\\log(\\hat{p}_{\\text{true class}})$$\n",
        "\n",
        "* If $\\hat{p}_{\\text{true class}} \\to 1$ ‚Üí loss $\\to 0$ ‚úÖ\n",
        "* If $\\hat{p}_{\\text{true class}} \\to 0$ ‚Üí loss $\\to \\infty$ ‚ùå (heavily penalized)\n",
        "\n",
        "Average over the dataset (+ regularization).\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Full Picture ‚Äî Logistic vs Softmax\n",
        "\n",
        "| | Logistic Regression | Softmax Regression |\n",
        "|---|---|---|\n",
        "| Classes | 2 (binary) | $K$ (multiclass) |\n",
        "| Scores | 1 score ‚Üí sigmoid | $K$ scores ‚Üí softmax |\n",
        "| Output | $P(y=1 \\mid \\mathbf{x})$ | $P(y=k \\mid \\mathbf{x})$ for all $k$ |\n",
        "| Loss | Binary cross-entropy | Multiclass cross-entropy |\n",
        "| Decision boundary | Linear hyperplane | $K$ linear hyperplanes |\n",
        "\n",
        "> üìù **Key idea:** Softmax is the natural generalization of logistic regression to $K$ classes. Same linear foundation, same cross-entropy loss ‚Äî just extended to output a full probability distribution over classes."
      ],
      "metadata": {
        "id": "OHalVwcVcxc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target  # 3 classes: 0,1,2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "softmax_clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"log_reg\", LogisticRegression(\n",
        "        multi_class=\"multinomial\",\n",
        "        solver=\"lbfgs\",\n",
        "        C=10.0,          # weaker regularization than default\n",
        "        max_iter=2000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "softmax_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = softmax_clf.predict(X_test)\n",
        "proba = softmax_clf.predict_proba(X_test)[:5]\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"First 5 predicted probability vectors:\\n\", np.round(proba, 4))\n",
        "\n",
        "lr = softmax_clf.named_steps[\"log_reg\"]\n",
        "print(\"Coef shape:\", lr.coef_.shape)      # (K, n_features)\n",
        "print(\"Intercept shape:\", lr.intercept_.shape)  # (K,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz492_2ZfG3T",
        "outputId": "45bef79e-39c1-4861-bb95-57a1f6de85f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n",
            "First 5 predicted probability vectors:\n",
            " [[9.982e-01 1.800e-03 0.000e+00]\n",
            " [1.000e-04 3.084e-01 6.915e-01]\n",
            " [8.010e-02 9.197e-01 2.000e-04]\n",
            " [3.760e-02 9.621e-01 2.000e-04]\n",
            " [9.993e-01 7.000e-04 0.000e+00]]\n",
            "Coef shape: (3, 4)\n",
            "Intercept shape: (3,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Softmax: Invariance to Constant Shifts\n",
        "\n",
        "---\n",
        "\n",
        "## Why Adding a Constant Doesn't Change Softmax (2-Line Proof)\n",
        "\n",
        "Let the original scores be $s_0, s_1, \\dots, s_{K-1}$. Softmax is:\n",
        "\n",
        "$$\\hat{p}_k = \\frac{e^{s_k}}{\\sum_j e^{s_j}}$$\n",
        "\n",
        "If we add a constant $c$ to every score:\n",
        "\n",
        "$$\\hat{p}'_k = \\frac{e^{s_k+c}}{\\sum_j e^{s_j+c}} = \\frac{e^c e^{s_k}}{e^c \\sum_j e^{s_j}} = \\frac{e^{s_k}}{\\sum_j e^{s_j}} = \\hat{p}_k$$\n",
        "\n",
        "> The $e^c$ cancels exactly. Probabilities are **identical**. ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## Practical Note ‚Äî Numerically Stable Softmax\n",
        "\n",
        "Because of this invariance, implementations compute:\n",
        "\n",
        "$$\\text{softmax}(\\mathbf{s}) = \\text{softmax}(\\mathbf{s} - \\max(\\mathbf{s}))$$\n",
        "\n",
        "This **prevents overflow** (huge exponentials from large scores) while keeping all probabilities identical.\n",
        "\n",
        "---\n",
        "\n",
        "## About the Scikit-Learn Warning\n",
        "\n",
        "The `multi_class=` parameter is being deprecated ‚Äî Softmax-style multinomial handling will become the default.\n",
        "\n",
        "Silence it by removing `multi_class=\"multinomial\"`:\n",
        "```python\n",
        "LogisticRegression(\n",
        "    solver=\"lbfgs\",\n",
        "    C=10.0,\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "> üìù **Key idea:** The invariance proof is not just a math curiosity ‚Äî it's why numerically stable Softmax implementations subtract the max score before exponentiating. Same math, no overflow."
      ],
      "metadata": {
        "id": "QDz2N17-gdUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 \"Training Models\" ‚Äî Lock-In Notes\n",
        "\n",
        "---\n",
        "\n",
        "## A) Where to Read (Pinpoint References in Order)\n",
        "\n",
        "| Section | Keywords |\n",
        "|---|---|\n",
        "| Linear Regression | `Linear Regression`, `MSE`, `cost function`, `theta` |\n",
        "| The Normal Equation | `Normal Equation`, `X^T X`, `inverse`, `pseudoinverse` |\n",
        "| Gradient Descent (all variants) | `Gradient Descent`, `learning rate`, `eta`, `Stochastic`, `Mini-batch`, `schedule` |\n",
        "| Polynomial Regression | `Polynomial Regression`, `PolynomialFeatures`, `degree` |\n",
        "| Learning Curves | `Learning Curves`, `bias`, `variance`, `underfitting`, `overfitting` |\n",
        "| Regularized Linear Models | `Regularized Linear Models`, `Ridge`, `Lasso`, `Elastic Net`, `alpha` |\n",
        "| Early Stopping | `Early Stopping`, `validation`, `patience` |\n",
        "| Logistic Regression | `Logistic Regression`, `sigmoid`, `log loss`, `cross entropy`, `decision boundary` |\n",
        "| Softmax Regression | `Softmax`, `multinomial`, `cross entropy` |\n",
        "\n",
        "---\n",
        "\n",
        "## B) Chapter 4 Lock-In Notes\n",
        "\n",
        "### 1) Linear Regression (Core Idea)\n",
        "\n",
        "**Model:**\n",
        "\n",
        "1 feature: $\\hat{y} = \\theta_0 + \\theta_1 x$\n",
        "\n",
        "$n$ features ‚Äî add bias feature $x_0 = 1$, so $\\mathbf{x} = [1, x_1, \\dots, x_n]$ and:\n",
        "\n",
        "$$\\hat{y} = \\mathbf{x}^\\top \\boldsymbol{\\theta}$$\n",
        "\n",
        "**Objective (MSE cost):**\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
        "\n",
        "> Training = choose $\\boldsymbol{\\theta}$ that minimizes MSE.\n",
        "\n",
        "**Matrix view:** Stack all instances into matrix $\\mathbf{X}$ (with bias column of 1s), targets into vector $\\mathbf{y}$, predictions: $\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\theta}$\n",
        "\n",
        "---\n",
        "\n",
        "### 2) Normal Equation (Closed-Form Solution)\n",
        "\n",
        "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{y}$$\n",
        "\n",
        "Jump straight to the MSE minimum ‚Äî no iterations.\n",
        "\n",
        "* **When it's great:** small/medium number of features\n",
        "* **When it's not:** huge feature counts (matrix operations become heavy)\n",
        "* **Practical note:** implementations use stable methods (pseudo-inverse / SVD) rather than a literal inverse\n",
        "\n",
        "---\n",
        "\n",
        "### 3) Gradient Descent Family (Iterative Optimization)\n",
        "\n",
        "**General update rule:**\n",
        "\n",
        "$$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta})$$\n",
        "\n",
        "$\\eta$ = learning rate: too small ‚Üí slow, too large ‚Üí overshoot / divergence\n",
        "\n",
        "| Variant | Instances per step | Path | Best for |\n",
        "|---|---|---|---|\n",
        "| Batch GD | All $m$ | Smooth, expensive | Small datasets |\n",
        "| SGD | 1 random | Noisy, fast | Large datasets |\n",
        "| Mini-batch GD | $b$ instances | Middle ground | **Default in practice** |\n",
        "\n",
        "**Batch GD gradient (for linear regression):**\n",
        "\n",
        "$$\\nabla_{\\boldsymbol{\\theta}}J(\\boldsymbol{\\theta}) = \\frac{2}{m}X_b^\\top(X_b\\boldsymbol{\\theta} - \\mathbf{y})$$\n",
        "\n",
        "**Learning schedule** (important with SGD/mini-batch): decrease $\\eta$ over time ‚Äî big steps early ‚Üí fast progress, smaller steps later ‚Üí settle near minimum.\n",
        "\n",
        "---\n",
        "\n",
        "### 4) Polynomial Regression (Curves Using Linear Models)\n",
        "\n",
        "**Trick:** make features nonlinear, keep the model linear in parameters.\n",
        "\n",
        "Degree $d$:\n",
        "\n",
        "$$\\hat{y} = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\cdots + \\theta_d x^d$$\n",
        "\n",
        "> üìù Polynomial regression is still \"linear\" because it's linear in the $\\theta$'s.\n",
        "\n",
        "**Risk:** higher degree ‚Üí higher flexibility ‚Üí higher overfitting risk.\n",
        "\n",
        "---\n",
        "\n",
        "### 5) Learning Curves (Diagnose Underfit vs Overfit)\n",
        "\n",
        "Plot training error and validation error vs training set size.\n",
        "\n",
        "| Pattern | Train error | Val error | Gap | Fix |\n",
        "|---|---|---|---|---|\n",
        "| **Underfitting** (high bias) | High | High | Small | Richer model, less regularization |\n",
        "| **Overfitting** (high variance) | Low | Higher | **Big** | Simplify, regularize, more data |\n",
        "\n",
        "---\n",
        "\n",
        "### 6) Regularized Linear Models (Control Overfitting by Shrinking Weights)\n",
        "\n",
        "**Ridge (L2):**\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{j=1}^{n}\\theta_j^2$$\n",
        "\n",
        "Larger $\\alpha$ ‚Üí weights shrink more ‚Üí variance ‚Üì, bias ‚Üë\n",
        "\n",
        "**Lasso (L1):**\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha \\sum_{j=1}^{n}|\\theta_j|$$\n",
        "\n",
        "Can drive some $\\theta_j$ to exactly 0 ‚Üí **feature selection** (sparse model)\n",
        "\n",
        "**Elastic Net (L1 + L2):**\n",
        "\n",
        "$$J(\\boldsymbol{\\theta}) = \\text{MSE}(\\boldsymbol{\\theta}) + \\alpha\\Big(r\\sum|\\theta_j| + \\frac{1-r}{2}\\sum\\theta_j^2\\Big)$$\n",
        "\n",
        "L1 helps sparsity, L2 helps stability. Use when features are correlated and you want \"some sparsity, but not unstable.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 7) Early Stopping (Regularization by Stopping at Best Validation)\n",
        "\n",
        "* Track validation error during training\n",
        "* Stop when validation error stops improving (with **patience**)\n",
        "* Keep the **best epoch weights** (lowest val error), not the last weights\n",
        "\n",
        "> **Why patience exists:** validation error can bounce slightly (noise), so you wait before deciding improvement is truly over.\n",
        "\n",
        "---\n",
        "\n",
        "### 8) Logistic Regression (Binary Classification)\n",
        "\n",
        "**Probability model:**\n",
        "\n",
        "$$\\hat{p} = \\sigma(z) = \\frac{1}{1+e^{-z}} = P(y=1 \\mid \\mathbf{x}), \\quad z = \\boldsymbol{\\theta}^\\top \\mathbf{x}$$\n",
        "\n",
        "**Decision rule:** predict 1 if $\\hat{p} \\geq 0.5$, else 0.\n",
        "Since $\\sigma(z) = 0.5 \\iff z = 0$, decision boundary: $\\boldsymbol{\\theta}^\\top \\mathbf{x} = 0$ (linear in feature space)\n",
        "\n",
        "**Training loss (log loss / cross-entropy):**\n",
        "* if $y=1$: $-\\log(\\hat{p})$\n",
        "* if $y=0$: $-\\log(1-\\hat{p})$\n",
        "\n",
        "Minimize average loss (+ regularization). `LogisticRegression` is regularized by default in scikit-learn.\n",
        "\n",
        "---\n",
        "\n",
        "### 9) Softmax Regression (Multiclass Logistic Regression)\n",
        "\n",
        "**Scores (one per class):** $s_k(\\mathbf{x}) = \\boldsymbol{\\theta}_k^\\top \\mathbf{x}$\n",
        "\n",
        "**Softmax probabilities:**\n",
        "\n",
        "$$\\hat{p}_k = \\frac{e^{s_k(\\mathbf{x})}}{\\sum_j e^{s_j(\\mathbf{x})}}$$\n",
        "\n",
        "**Predict class:** $\\arg\\max_k \\hat{p}_k$ (equivalently $\\arg\\max_k s_k$)\n",
        "\n",
        "**Cross-entropy loss:** $-\\log(\\hat{p}_{\\text{true class}})$ for each example\n",
        "\n",
        "**Numerical stability trick:** softmax doesn't change if you add the same constant to all scores, so compute:\n",
        "\n",
        "$$\\text{softmax}(\\mathbf{s}) = \\text{softmax}(\\mathbf{s} - \\max(\\mathbf{s}))$$\n",
        "\n",
        "(prevents overflow from huge exponentials)\n",
        "\n",
        "---\n",
        "\n",
        "## C) Code Patterns to Own (Templates)\n",
        "\n",
        "### 1) Linear Regression ‚Äî Normal Equation Style\n",
        "```python\n",
        "X_b = np.c_[np.ones((m, 1)), X]           # add bias column\n",
        "theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "```\n",
        "\n",
        "### 2) Book-Style Pipelines (Safest Default)\n",
        "```python\n",
        "Pipeline([\n",
        "    (\"poly\",    PolynomialFeatures(degree=d, include_bias=False)),\n",
        "    (\"scaler\",  StandardScaler()),\n",
        "    (\"model\",   Ridge() / Lasso() / ElasticNet() / LogisticRegression())\n",
        "])\n",
        "```\n",
        "\n",
        "> **Rule of thumb:** if you use GD/SGD or regularization, **scale features**.\n",
        "\n",
        "### 3) Early Stopping (Mental Model)\n",
        "```python\n",
        "# Train epoch by epoch\n",
        "# Track validation RMSE (or log loss)\n",
        "# Save best weights\n",
        "# Stop after patience runs out\n",
        "# Restore best weights\n",
        "```"
      ],
      "metadata": {
        "id": "oYTZlVoihoAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Chapter 4 ‚Äî Exercises: Lock-In Q&A\n",
        "\n",
        "---\n",
        "\n",
        "**Q1: What Linear Regression training algorithm can you use if you have a training set with millions of features?**\n",
        "\n",
        "Use an iterative method such as **Stochastic Gradient Descent** or **Mini-batch Gradient Descent**. The Normal Equation is impractical at that scale ‚Äî inverting $(\\mathbf{X}^\\top \\mathbf{X})$ becomes computationally prohibitive with millions of features.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2: Suppose the features in your training set have very different scales. What algorithms might suffer, and what can you do?**\n",
        "\n",
        "* **Gradient Descent** (Batch/SGD/Mini-batch) becomes very slow ‚Äî the cost function becomes poorly conditioned (long, narrow valleys)\n",
        "* **Regularized models** (Ridge/Lasso/Elastic Net/Logistic Regression) behave poorly ‚Äî the penalty is affected by scale, so some features dominate unfairly\n",
        "\n",
        "> ‚úÖ Fix: Scale features with `StandardScaler`, ideally inside a `Pipeline`.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3: Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?**\n",
        "\n",
        "**No.** Logistic regression's log-loss objective is **convex** ‚Äî it has a single global optimum, so there are no problematic local minima.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4: Do all Gradient Descent algorithms lead to the same model if you let them run long enough?**\n",
        "\n",
        "For convex problems (linear/logistic regression):\n",
        "* **Batch GD** converges to the global optimum (with a suitable learning rate)\n",
        "* **SGD/Mini-batch** may keep oscillating around the optimum if the learning rate doesn't decay\n",
        "\n",
        "> ‚úÖ Fix: Use a **learning schedule** (decreasing $\\eta$) to make SGD/mini-batch truly converge.\n",
        "\n",
        "---\n",
        "\n",
        "**Q5: You use Batch GD and notice the validation error consistently goes up at every epoch. What's happening? How do you fix it?**\n",
        "\n",
        "Most likely **overfitting** ‚Äî training error keeps improving but generalization worsens.\n",
        "\n",
        "Fixes: early stopping, regularization (Ridge/Lasso/Elastic Net), reduce model complexity, get more data.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6: Is it a good idea to stop Mini-batch GD immediately when the validation error goes up?**\n",
        "\n",
        "**No.** Mini-batch/SGD validation error is **noisy** and can rise temporarily due to randomness. Use **patience** ‚Äî wait several epochs before deciding ‚Äî and always keep the best validation weights.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7: Which GD algorithm reaches the vicinity of the optimal solution fastest? Which actually converges? How can you make the others converge?**\n",
        "\n",
        "* **Fastest to get near optimum:** SGD (and often mini-batch in practice)\n",
        "* **Most likely to converge smoothly:** Batch GD\n",
        "* **Make SGD/mini-batch converge:** use a learning schedule (decrease $\\eta$ over time)\n",
        "\n",
        "---\n",
        "\n",
        "**Q8: You're using Polynomial Regression and notice a large gap between training and validation error. What's happening? Three ways to fix it?**\n",
        "\n",
        "This is **overfitting (high variance)**.\n",
        "\n",
        "Three fixes:\n",
        "1. **Lower the polynomial degree** (simplify the model)\n",
        "2. **Add regularization** (Ridge / Lasso / Elastic Net)\n",
        "3. **Get more training data** (often reduces the gap)\n",
        "\n",
        "*(Also acceptable: early stopping, feature selection.)*\n",
        "\n",
        "---\n",
        "\n",
        "**Q9: You use Ridge Regression and notice training error and validation error are almost equal and fairly high. High bias or high variance? Increase Œ± or reduce it?**\n",
        "\n",
        "That's **high bias (underfitting)** ‚Äî the model is too constrained.\n",
        "\n",
        "> ‚úÖ Fix: **Reduce $\\alpha$** (less regularization) and/or use a more expressive model or better features.\n",
        "\n",
        "---\n",
        "\n",
        "**Q10: Why use Ridge instead of plain Linear Regression? Lasso instead of Ridge? Elastic Net instead of Lasso?**\n",
        "\n",
        "| Choice | Why |\n",
        "|---|---|\n",
        "| **Ridge over plain LR** | Reduces overfitting/variance by shrinking weights; helps with multicollinearity |\n",
        "| **Lasso over Ridge** | Can set some coefficients **exactly to 0** ‚Üí automatic feature selection (sparse model) |\n",
        "| **Elastic Net over Lasso** | More stable when features are correlated; can keep groups of related features that Lasso might drop arbitrarily |\n",
        "\n",
        "---\n",
        "\n",
        "**Q11: You want to classify pictures as outdoor/indoor AND daytime/nighttime. Two Logistic Regression classifiers or one Softmax Regression?**\n",
        "\n",
        "Use **two separate Logistic Regression classifiers** ‚Äî one per binary label.\n",
        "\n",
        "Softmax is for a **single multiclass label with mutually exclusive classes**. Here you have two independent binary decisions, so two binary classifiers is the correct framing.\n",
        "\n",
        "---\n",
        "\n",
        "**Q12: Implement Batch Gradient Descent with early stopping for Softmax Regression (without Scikit-Learn).**\n",
        "\n",
        "> See code cell below ‚Äî pure NumPy implementation of Softmax Regression trained with Batch GD + early stopping (patience, best validation weights)."
      ],
      "metadata": {
        "id": "rWFAL_7Rjrzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(logits: np.ndarray) -> np.ndarray:\n",
        "    z = logits - logits.max(axis=1, keepdims=True)  # stability\n",
        "    exp = np.exp(z)\n",
        "    return exp / exp.sum(axis=1, keepdims=True)\n",
        "\n",
        "def one_hot(y: np.ndarray, K: int) -> np.ndarray:\n",
        "    Y = np.zeros((y.shape[0], K))\n",
        "    Y[np.arange(y.shape[0]), y] = 1.0\n",
        "    return Y\n",
        "\n",
        "def standardize_fit(X: np.ndarray):\n",
        "    mu = X.mean(axis=0, keepdims=True)\n",
        "    sigma = X.std(axis=0, keepdims=True) + 1e-12\n",
        "    return mu, sigma\n",
        "\n",
        "def standardize_transform(X: np.ndarray, mu: np.ndarray, sigma: np.ndarray):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "def add_bias(X: np.ndarray) -> np.ndarray:\n",
        "    return np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "def cross_entropy_loss(Xb: np.ndarray, y: np.ndarray, Theta: np.ndarray, l2: float) -> float:\n",
        "    probs = softmax(Xb @ Theta)\n",
        "    m = Xb.shape[0]\n",
        "    loss = -np.log(probs[np.arange(m), y] + 1e-15).mean()\n",
        "    loss += 0.5 * l2 * np.sum(Theta[1:, :] ** 2)  # don't penalize bias row\n",
        "    return loss\n",
        "\n",
        "def train_softmax_bgdearly(\n",
        "    X_train: np.ndarray, y_train: np.ndarray,\n",
        "    X_val: np.ndarray, y_val: np.ndarray,\n",
        "    lr: float = 0.1,\n",
        "    n_epochs: int = 2000,\n",
        "    l2: float = 1e-3,\n",
        "    patience: int = 20,\n",
        "    tol: float = 1e-6,\n",
        "    seed: int = 42\n",
        "):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    K = int(max(y_train.max(), y_val.max()) + 1)\n",
        "\n",
        "    # Standardize features\n",
        "    mu, sigma = standardize_fit(X_train)\n",
        "    Xtr = standardize_transform(X_train, mu, sigma)\n",
        "    Xva = standardize_transform(X_val, mu, sigma)\n",
        "\n",
        "    Xtr_b = add_bias(Xtr)\n",
        "    Xva_b = add_bias(Xva)\n",
        "\n",
        "    n = Xtr_b.shape[1]\n",
        "    Theta = rng.normal(0, 0.01, size=(n, K))\n",
        "\n",
        "    best_Theta = Theta.copy()\n",
        "    best_val = float(\"inf\")\n",
        "    patience_left = patience\n",
        "\n",
        "    Ytr = one_hot(y_train, K)\n",
        "    m = Xtr_b.shape[0]\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        probs = softmax(Xtr_b @ Theta)                 # (m, K)\n",
        "        grad = (Xtr_b.T @ (probs - Ytr)) / m           # (n, K)\n",
        "        grad[1:, :] += l2 * Theta[1:, :]               # L2 (skip bias)\n",
        "\n",
        "        Theta -= lr * grad\n",
        "\n",
        "        val_loss = cross_entropy_loss(Xva_b, y_val, Theta, l2)\n",
        "\n",
        "        if val_loss < best_val - tol:\n",
        "            best_val = val_loss\n",
        "            best_Theta = Theta.copy()\n",
        "            patience_left = patience\n",
        "        else:\n",
        "            patience_left -= 1\n",
        "            if patience_left == 0:\n",
        "                break\n",
        "\n",
        "    return {\"Theta\": best_Theta, \"mu\": mu, \"sigma\": sigma, \"epochs_ran\": epoch + 1, \"best_val_loss\": best_val}"
      ],
      "metadata": {
        "id": "b2hoLREzfNC4"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}